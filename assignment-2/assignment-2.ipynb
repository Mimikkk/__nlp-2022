{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytywanie danych\n",
    "Paczka Sklearn (Scikit Learn) to kolejne bardzo popularne narzędzie do uczenia maszynowego. Posiada bardzo przejrzyste API i spore wsparcie (scikit-learn.org/). W dzisiejszych zadaniach skupimy się na ładowaniu zbiorów danych, ich transformacji oraz algorytmach klasyfikacji. Podczas dzisiejszych laboratoriów wykorzystamy:\n",
    "<ul>\n",
    "    <li>NLTK - udostępniające metody prostego przetwarzania tekstu (tokenizacja, lematyzacja, stemming)</li>\n",
    "    <li>Sklearn - paczkę do uczenia maszynowego</li>\n",
    "    <li>Pandas - bibliotekę do wczytywania i obsługi zbiorów danych</li>\n",
    "</ul>\n",
    "<span style=\"color: #ff0000\">Ponieważ część kodu jest już stworzona, w każdym zadaniu wyszczególnione są numery linii, w których należy wprowadzić modyfikacje, aby rozwiązać zadanie. Jeśli nie widzisz numeracji linii w kodzie w otwartym notebooku - możesz włączyć tę funkcjonalność poprzez wybór View -> toggle line numbers w górnym menu.</span><br/><br/>\n",
    "Najpierw wczytajmy dane tekstowe ze zbioru, w którym posiadamy zestaw wiadomości e-mail oznaczonych jako spamowe lub niespamowe.\n",
    "Ponieważ będziemy rozwiązywać problem klasyfikacji, oddzielamy dane do trenowania klasyfikatora oraz do weryfikacji jego jakości.\n",
    "<br/>\n",
    "\n",
    "<strong>Przeanalizuj i uruchom poniższy fragment kodu.</strong> Załaduje on odpowiednie dane do dwóch obiektów:\n",
    "<ol>\n",
    "<li>train: zbiór treningowy - dokumenty na których nauczymy klasyfikator</li>\n",
    "<li>test: zbiór testowy - dokumenty na których przetestujemy klasyfikator</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementów w zbiorze treningowym/testowym: 1624/733\n",
      "Liczność klas w zbiorze treningowym/testowym: ham     1111\n",
      "spam     513\n",
      "Name: label, dtype: int64/ham     517\n",
      "spam    216\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "  label                                               text  label_num\n0   ham  Re: What to choose for Core i5 64 bits?>>> If ...          0\n1  spam  Strictly Private.Gooday, With warm heart my fr...          1\n2   ham  Re: Flash is open?On Sat, 15 May 2010 00:27:32...          0\n3   ham  Re: Alsa/Redhat 8 compatabilityMatthias Saou (...          0\n4  spam  Hey hibody, Save 80% today Lixi Eights followi...          1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n      <th>label_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Re: What to choose for Core i5 64 bits?&gt;&gt;&gt; If ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>spam</td>\n      <td>Strictly Private.Gooday, With warm heart my fr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>Re: Flash is open?On Sat, 15 May 2010 00:27:32...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Re: Alsa/Redhat 8 compatabilityMatthias Saou (...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>spam</td>\n      <td>Hey hibody, Save 80% today Lixi Eights followi...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "# --- Ładowanie danych i oddzielanie zbioru treningowego od testowego ---\n",
    "import scipy.sparse\n",
    "full_dataset = pandas.read_csv('resources/spam-emails.csv', encoding='utf-8')\n",
    "\n",
    "# ponieważ nazwy kategorii zapisane są z użyciem stringów: \"ham\"/\"spam\",\n",
    "# wykonujemy mapowanie tych wartości na liczby, co będzie potrzebne do wykonania klasyfikacji.\n",
    "full_dataset['label_num'] = full_dataset.label.map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# ustaw seed na 0, aby zapewnić powtarzalność eksperymentu\n",
    "np.random.seed(0)\n",
    "train_indices = np.random.rand(len(full_dataset)) < 0.7\n",
    "train: pandas.DataFrame = full_dataset[train_indices]\n",
    "test: pandas.DataFrame = full_dataset[~train_indices]\n",
    "\n",
    "# --- Wyświetlanie statystyk ---\n",
    "print(f\"Elementów w zbiorze treningowym/testowym: {len(train)}/{len(test)}\")\n",
    "print(f\"Liczność klas w zbiorze treningowym/testowym: {train.label.value_counts()}/{test.label.value_counts()}\")\n",
    "\n",
    "# wyświetl próbkę danych\n",
    "full_dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformacja danych\n",
    "Aby zastosować większość algorytmów uczenia maszynowego - dane wejściowe muszą być reprezentowane jako wektory liczb. Wykorzystajmy zatem narzędzia dostarczone przez Scikit-learn do tego celu. Użyjmy klasy **CountVectorizer()**, aby podzielić poszczególne dokumenty na słowa, a następnie stworzyć reprezentację \"bag of words\"\n",
    "Dla przypomnienia - \"bag of words\" tworzony jest w nastepujący sposób:\n",
    "<ol>\n",
    "<li>Przeglądamy wszystkie dostępne dokumenty i tworzymy listę wszystkich unikalnych słów jakie napotkaliśmy (słownik).</li>\n",
    "<li>Stworzona lista wyznacza nam wektor cech - każda pozycja w takim wektorze oznacza jedno z napotkanych słów.</li>\n",
    "<li>Każdy z dokumentów mapowany jest na wektor cech poprzez zapisanie ile razy każde ze słów dokumentu wystąpiło w nim.</li>\n",
    "</ol>\n",
    "Przykład wektoryzacji znajduje się poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PRZYKŁAD ------------------------------------------\n",
    "# np. Dla dwóch dokumentów:\n",
    "# Dokument 1: Ala ma kota i ma psa \n",
    "# Dokument 2: Kot ma Alę\n",
    "\n",
    "# Poszczególne kroki wyglądają następująco:\n",
    "# Lista unikalnych słów:                 [Ala, ma, kota, i, psa, Kot, Alę] \n",
    "# Szablon wektora cech:                  [  0,  0,    0, 0,   0,   0,   0] - wektor jest tyluelementowy, ile mamy  unikalnych słów \n",
    "# Osadzenie dokumentu 1 jako wektor cech: [  1,  2,    1, 1,   1,   0,   0] - słowo \"ma\" pojawia się w dok. 2 razy, \"kot\" i \"Alę\" - wcale\n",
    "# Analogicznie dokument 2:               [  0,  1,    0, 0,   0,   1,   1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozmiar stworzonej macierzy: (1624, 37325)\n",
      "Liczba dokumentów: 1624\n",
      "Rozmiar wektora bag-of-words: 37325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "# Stwórz macierz liczbową z danych.\n",
    "# W wierszach mamy kolejne dokumenty,\n",
    "# w kolumnach kolejne pola wektora cech odpowiadające unikalnym słowom (bag of words)\n",
    "X_train_counts = vectorizer.fit_transform(train['text'])\n",
    "# analogicznie jak wyżej — dla zbioru testowego.\n",
    "X_test_counts = vectorizer.transform(test['text'])\n",
    "\n",
    "# Wyświetl rozmiar macierzy.\n",
    "# Pierwsze pole — liczba dokumentów,\n",
    "# drugie — liczba cech (stała dla wszystkich dokumentów)\n",
    "print(f\"Rozmiar stworzonej macierzy: {X_train_counts.shape}\")\n",
    "print(f\"Liczba dokumentów: {X_train_counts.shape[0]}\")\n",
    "print(f\"Rozmiar wektora bag-of-words: {X_train_counts.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uwaga:\n",
    "Na zbiorze treningowym użyto funkcji - fit_transform(), na testowym - transform(). <br/>\n",
    "**Dlaczego?** fit_transform() wykonuje dwie operacje - tworzy i zapisuje listę wszystkich unikalnych słów (słownik) oraz zamienia dokument na wektor o długości takiej jak słownik. transform() natomiast wykorzystuje istniejący już słownik i wykonuje z jego użyciem transformację do wektora. \n",
    "<br/>\n",
    "Ponieważ zbiór treningowy jest zazwyczaj liczniejszy - z reguły znajdziemy w nim więcej różnych słów. Ponadto, wszystkie słowa, które mogą pomóc w klasyfikacji i tak muszą znaleźć się w zbiorze treningowym aby móc się nauczyć ich wykorzystania. Nie nadpisuje się zatem słownika za pomocą zbioru testowego i tworzy się go tylko raz - podczas treningu, wykorzystując go następnie do tworzenia nowych wektorów z nieobserwowanych podczas treningu dokumentów.\n",
    "\n",
    "# Zadanie 1 (1 punkt):\n",
    "Jak się pewnie domyślasz - reprezentacja bag-of-words będzie miała bardzo wiele zer w wygenerowanych macierzach (macierzach, w których w poszczególnych wierszach będziemy mieli poszczególne dokumenty, a w kolumnach wektory słów reprezentacji bag of words). Rozmiar macierzy z poprzedniego listingu pokazuje, że każdy dokument opisany jest wektorem 37325 pozycji, ponieważ tyle różnych słów zostało wykrytych po analizie wszystkich dokumentów treningowych. Większość dokumentów analizowanych osobbno zawierać będzie pewnie co najwyżej kilkadziesiąt/kilkaset różnych słów.\n",
    "\n",
    "***Zadanie: Napisz fragment kodu, który zliczy:***\n",
    "<ol>\n",
    "    <li><strong>jaki procent macierzy X_train_counts ma elementy o wartości różnej od zera</strong></li>\n",
    "    <li><strong>ile tokenów (łącznie, nie tylko unikalne) występuje w macierzy X_train_counts?</strong></li>\n",
    "</ol>\n",
    "Wskazówka - ponieważ zer w tej macierzy jest istotnie dużo - dane po transformacji CountVectorizerem trzymane są w specjalnym formacie, w którym zapisuje się tylko elementy mające wartości różne od zera, w tzw. macierzy rzadkiej (sparse matrix). Aby przeiterować po takiej macierzy, można wykorzystać następujące fragmenty kodu:\n",
    "\n",
    "<strong>cx = X_train_counts.tocoo()</strong> - transformuj macierz do reprezentacji koordynatowej (patrz niżej) <br/>\n",
    "<strong>for doc_id, word_id, count in zip(cx.row, cx.col, cx.data):</strong> - pozwala ona na iterowanie po wszystkich niezerowych elementach, w każdym kroku otrzymując 3 zmienne - numer wiersza (numer dokumentu), numer kolumny (identyfikator słowa ze słownika) oraz licznik mówiący ile razy dane słowo wystąpiło w danym dokumencie. <span style=\"color: #ff0000\">(Do wykonania zadania musisz zaktualizować linijki 3, 7 i 8)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W datasecie znajduje się: 435292 tokenów.\n",
      "Macierz posiada 0.394% niezerowych elementów.\n"
     ]
    }
   ],
   "source": [
    "# tu zapisz liczbę wszystkich tokenów w macierzy\n",
    "count_tokens = 0\n",
    "# tu zapisz ilość elementów niezerowych w macierzy\n",
    "count_nonzero = 0\n",
    "# tu zapisz ilość komórek w macierzy (ilość wierszy * ilość kolumn, rozważ użycie pola 'shape' na macierzy X_train_counts)\n",
    "count_all = np.multiply(*X_train_counts.shape)\n",
    "\n",
    "#iteracja po elementach niezerowych\n",
    "cx = X_train_counts.tocoo()\n",
    "for (doc_id, word_id, count) in zip(cx.row, cx.col, cx.data):\n",
    "  count_tokens += count\n",
    "  count_nonzero += count > 0\n",
    "\n",
    "print(f\"W datasecie znajduje się: {count_tokens} tokenów.\")\n",
    "print(f\"Macierz posiada {100.0 * count_nonzero / count_all:.3f}% niezerowych elementów.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <strong>Oczekiwany rezultat:</strong> <br/>\n",
    "Mniej niż 1% elementów niezerowych (!) <br/>\n",
    "Ponad 400000 tokenów\n",
    "</div>\n",
    "\n",
    "# Zadanie 2 (1 punkt) - słowa charakteryzujące klasy\n",
    "\n",
    "Wykorzystajmy macierz X_train_counts wykorzystywaną w poprzednim zadaniu, a także etykiety kategorii, aby stworzyć listy najczęściej występuących słów w danych kategoriach. <br/><br/>\n",
    "Aby ułatwić zadanie, utworzono większość funkcji **get_top_occuring_words()** tworzącej taki ranking<br/>\n",
    "**Zadanie 2a (0.5 punktu)**: twoim zadaniem jest zaktualizowanie wartości pola: **category_word_counts[category][word]**, tak, aby poprawnie zliczyć ile razy dane słowo wystąpiło w kategorii. <span style=\"color: #ff0000\">(zaktualizuj linijkę 20)</span>\n",
    "<br/>\n",
    "Czy najczęstsze słowa pozwalają rozdzielić kategorie SPAM od HAM?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham: ['the', 'to', 'of', 'and', 'is', 'in', 'it', 'that', 'for', 'you', 'on', 'with']\n",
      "spam: ['the', 'of', 'to', 'and', 'in', 'you', 'nbsp', 'for', 'is', 'your', 'this', 'as']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_top_occurring_words(X_train_counts, how_many_words, vectorizer, train):\n",
    "  # stwórz mapowanie pozycji wektora bag-of-words na konkretne słowa\n",
    "  id_to_word = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "  cx = X_train_counts.tocoo()\n",
    "\n",
    "  # słownik, w którym przeprowadzimy zliczanie\n",
    "  category_word_counts = defaultdict(lambda: defaultdict(int))\n",
    "  for (doc_id, word_id, count) in zip(cx.row, cx.col, cx.data):\n",
    "    category = train.iloc[doc_id]['label']\n",
    "    word = id_to_word[word_id]\n",
    "    category_word_counts[category][word] += count\n",
    "\n",
    "  # wyświetl nazwy kategorii i n-najczęściej występujących w nich słów\n",
    "  for category_name in category_word_counts:\n",
    "    # posortowany dict() słowo -> liczność, wg liczności, malejąco\n",
    "    sorted_cat = sorted(category_word_counts[category_name], key=category_word_counts[category_name].get, reverse=True)\n",
    "    # wyświetl nazwę kategorii i top n słów\n",
    "    print(f\"{category_name}: {sorted_cat[:how_many_words]}\")\n",
    "\n",
    "# wywołanie funkcji\n",
    "get_top_occurring_words(X_train_counts, 12, vectorizer, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wektoryzacja Tf-Idf\n",
    "\n",
    "Po wykonaniu zadania 2a widzimy, że najczęściej występujące słowa w każdej kategorii mają niewielką użytezczność (pasują do każdej kategorii). Aby sprawić, żeby na czele rankingu znalazły się słowa charakterystyczne dla danej klasy, możemy użyć metody Tf-idf. <br/>\n",
    "**Zadanie 2b:\n",
    "Nadpisz wartości X_train_counts oraz X_test_counts wykorzystując w tym celu TfidfVectorizer** (http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) zamiast CountVectorizer, ustaw parametr max_df na 0.4 (tzn. ignoruj słowa, które występują w więcej niż 40% dokumentów). Następnie wykonaj stworzoną w zadaniu 2 funkcję get_top_occuring_words(), aby sprawdzić, czy ranking najważniejszych słów się zmienił. Czy zmieniony zestaw słów lepiej reprezentuje kategorie? <span style=\"color: #ff0000\">(zaktualizuj linie 3, 4, 5)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham: ['debian', 'org', 'lists', '20', 'unsubscribe', 'linux', 'net', 'wrote', 'list', 'my', 'can', 'www']\n",
      "spam: ['nbsp', 'our', '20', 'click', 'here', 'spam', '2009', 'content', 'hibody', 'we', 'free', 'all']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.4)\n",
    "# Stwórz macierz wektorów. W wierszach mamy kolejne dokumenty, w kolumnach kolejne pola wektora cech odpowiadające unikalnym słowom\n",
    "X_train_counts: scipy.sparse.csr_matrix = vectorizer.fit_transform(train['text'])\n",
    "# analogicznie dla zbioru testowego.\n",
    "X_test_counts: scipy.sparse.csr_matrix = vectorizer.transform(test['text'])\n",
    "\n",
    "# wywołanie funkcji\n",
    "get_top_occurring_words(X_train_counts, 12, vectorizer, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 3 - Stemming i lematyzacja (1 punkt)\n",
    "Często istotne słowa występują w wielu odmianach (szczególnie w językach fleksyjnych, takich jak nasz), np: university - universities ; pay - paid - paying - pays . Wielość odmian słów ma swoje przełożenie na rozmiar słownika.\n",
    "<br/><br/>\n",
    "W niektórych warunkach, w szczególności:\n",
    "<ul>\n",
    "<li>Kiedy mamy ograniczoną pamięć</li>\n",
    "<li>Kiedy ważny jest dla nas czas działania algorytmu</li>\n",
    "<li>Kiedy istnieje ryzyko przeuczenia</li>\n",
    "</ul>\n",
    "warto rozważyć znormalizowanie słów, tak, aby zmniejszyć rozmiar słownika, a co za tym idzie wymagania pamięciowe (a co za tym idzie - czas treningu/klasyfikacji). Ograniczenie rozmiaru słownika może też zapobiec przeuczeniu. Normalizację możemy wykonać np. poprzez zastosowanie stemmingu lub lematyzacji dla poszczególnych wyrazów.\n",
    "<br/>\n",
    "<strong>Zadanie 3a (0.5 punktu)</strong>: Z użyciem biblioteki NLTK wykonaj zarówno lematyzację (używając WordNetLemmatizer) jak i stemming (używając PorterStemmer) tekstu zawartego w sample_text. Uwaga - lematyzator opcjonalnie wymaga pos-tagu dla tokenu. Przekaż do funkcji lematyzującej zmienną current_word_postag jako drugi argument. <span style=\"color: #ff0000\">(zaktualizuj linie 20, 21, 34, 35)</span>\n",
    "\n",
    "<strong>Zadanie 3b (0.5 punktu)</strong>: O ile zmniejszyła się liczba unikalnych słów po zastosowaniu lematyzacji? Odpowiedź zawrzyj w komentarzu. <span style=\"color: #ff0000\">(linijki 43:45)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bazowy tekst:        \n",
      "\tThere are some cheaper alternatives for buying the red trousers. There is a discount, it is so cheap!\n",
      "\n",
      "Wystemowany tekst:   \n",
      "\tthere are some cheaper altern for buy the red trouser . there is a discount , it is so cheap !\n",
      "\n",
      "Zlematyzowany tekst: \n",
      "\tThere be some cheap alternative for buy the red trouser . There be a discount , it be so cheap !\n",
      "\n",
      "Liczba unikalnych tokenów: 19\n",
      "Liczba unikalnych stemów:  19\n",
      "Liczba unikalnych lematów: 17\n",
      "Różnica między liczbą unikalnych tokenów i stemów:  0\n",
      "Różnica między liczbą unikalnych tokenów i lematów: 2\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# Lematyzator wymaga, aby dla danego słowa podać mu, czy jest to czasownik, rzeczownik czy inny POS-tag.\n",
    "# Funkcja jest adapterem tagów nadanych przez funkcję pos_tag do tagów wymaganych przez lematyzator\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "  match treebank_tag:\n",
    "    case tag if tag.startswith('J'):\n",
    "      return wordnet.ADJ\n",
    "    case tag if tag.startswith('V'):\n",
    "      return wordnet.VERB\n",
    "    case tag if tag.startswith('R'):\n",
    "      return wordnet.ADV\n",
    "    case _:\n",
    "      return wordnet.NOUN\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sample_text = \"There are some cheaper alternatives for buying the red trousers. There is a discount, it is so cheap!\"\n",
    "\n",
    "# dzielimy tekst na słowa\n",
    "tokenized = word_tokenize(sample_text)\n",
    "# nadajemy pos-tagi (rzeczownik, czasownik przymiotnik...) każdemu słowu\n",
    "pos_tokens = nltk.pos_tag(tokenized)\n",
    "\n",
    "# Zlematyzowane słowa\n",
    "lemmatized = [lemmatizer.lemmatize(token, get_wordnet_pos(tag))\n",
    "              for (token, tag) in nltk.pos_tag(tokenized)]\n",
    "# Wystemowane słowa\n",
    "stemmed = [stemmer.stem(token) for (token, _) in nltk.pos_tag(tokenized)]\n",
    "\n",
    "print(f\"{'Bazowy tekst:':<21}\\n\\t{sample_text}\\n\")\n",
    "print(f\"{'Wystemowany tekst:':<21}\\n\\t{' '.join(stemmed)}\\n\")\n",
    "print(f\"{'Zlematyzowany tekst:':<21}\\n\\t{' '.join(lemmatized)}\\n\")\n",
    "\n",
    "# Ile unikalnych tokenów znajduje się w tekście bazowym?:\n",
    "print(f\"{'Liczba unikalnych tokenów:':<27}{len(set(tokenized))}\")\n",
    "# Ile unikalnych tokenów znajduje się w tekście wystemowanym?:\n",
    "print(f\"{'Liczba unikalnych stemów:':<27}{len(set(stemmed))}\")\n",
    "# Ile unikalnych tokenów w tekście zlematyzowanym?:\n",
    "print(f\"{'Liczba unikalnych lematów:':<27}{len(set(lemmatized))}\")\n",
    "\n",
    "# Różnica ilości unikalnych tokenów między tekstem bazowym a wystemowanym:\n",
    "print(f\"{'Różnica między liczbą unikalnych tokenów i stemów:':<52}{abs(len(set(tokenized)) - len(set(stemmed)))}\")\n",
    "# Różnica ilości unikalnych tokenów między tekstem bazowym a zlematyzowanym:\n",
    "print(f\"{'Różnica między liczbą unikalnych tokenów i lematów:':<52}{abs(len(set(tokenized)) - len(set(lemmatized)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Zadanie 4 (1 punkt) - klasyfikacja i interpretacja wyników\n",
    "Mając już dobrą reprezentację danych i wiedząc jak działa normalizacja - możemy klasyfikować! <br/>\n",
    "Istnieje wiele algorytmów, które dobrze radzą sobie z klasyfikacją tekstu, kilka przykładów to: \n",
    "<ul>\n",
    "<li>Naiwny klasyfikator Bayesa</li>\n",
    "<li>Maszyna wektorów nośnych - SVM</li>\n",
    "<li>Sieci neuronowe</li>\n",
    "</ul>\n",
    "O sieciach neuronowych więcej powiemy na jednych z przyszłych laboratoriów. <br/>\n",
    "<strong>Zadanie 4a (0.5 punktu)</strong> Wykorzystując przetworzoną postać danych: X_train_counts, X_test_counts z poprzednich zadań oraz dokumentację sklearn, zaimplementuj klasyfikację z użyciem naiwnego klasyfikatora Bayesa (MultinomialNB). <span style=\"color: #ff0000\">(zaktualizuj linie 7, 9, 12)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ile elementów testowych udało się poprawnie zaklasyfikować?\n",
      "Poprawnie zaklasyfikowano: 88.404% 648/733 elementów\n",
      "Szczegółowy raport (per klasa)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.86      1.00      0.92       517\n",
      "        spam       1.00      0.61      0.76       216\n",
      "\n",
      "    accuracy                           0.88       733\n",
      "   macro avg       0.93      0.80      0.84       733\n",
      "weighted avg       0.90      0.88      0.87       733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Funkcja pomocnicza zamieniająca identyfikatory numeryczne na tekstowe\n",
    "def labels_as_strings(vector_of_indices):\n",
    "  return ['ham' if ind == 0 else 'spam' for ind in vector_of_indices]\n",
    "\n",
    "# Stwórz klasyfikator\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Wytrenuj klasyfikatorn\n",
    "y = X_train_counts\n",
    "nb.fit(X_train_counts, train['label_num'])\n",
    "\n",
    "print(\"Ile elementów testowych udało się poprawnie zaklasyfikować?\")\n",
    "results: np.ndarray = nb.predict(X_test_counts)\n",
    "accuracy = len(test[results == test['label_num']])\n",
    "\n",
    "print(f\"Poprawnie zaklasyfikowano: {100 * accuracy / len(results):.3f}% {accuracy}/{len(results)} elementów\")\n",
    "\n",
    "# Testowanie klasyfikatora — szerokie podsumowanie uwzględniające miary:\n",
    "# - precision\n",
    "# - recall\n",
    "# - f1\n",
    "print(\"Szczegółowy raport (per klasa)\")\n",
    "print(classification_report(\n",
    "  labels_as_strings(test['label_num']),\n",
    "  labels_as_strings(nb.predict(X_test_counts)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 4b (0.5 punktu)\n",
    "Po analizie szczegółowego raportu z zadania 4a - odpowiedz na poniższe pytania i zapisz odpowiedzi w komentarzu:**\n",
    "<ol>\n",
    "<li>Która miara mówi nam o tym, jak wiele spośród elementów uznanych za spam rzeczywiście jest spamem?</li>\n",
    "<li>Która miara mówi nam o tym, jak wiele spośród wszystkich elementów rzeczywiście będących spamem zostało wykrytych jako spam?</li>\n",
    "<li>Która kategoria została w ogólnym rozrachunku lepiej rozpoznana przez klasyfikator, jeśli zależy nam bardziej na tym, żeby klasyfikator, jeśli mówi, że coś należy do danej klasy, raczej się w tym nie mylił, niż żeby wykrył wszystkie elementy klasy?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# odp zad 4.1: precision\n",
    "# odp zad 4.2: recall\n",
    "# odp zad 4.3: spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn jest bardzo wdzięcznym narzędziem, w którym proces klasyfikacji możemy wykonać w zaledwie kilku linijkach. Bardzo przydatną klasą jest klasa Pipeline, która definiuje sekwencję kroków, które wykonujemy wywołując metodę fit().\n",
    "W naszym przypadku mamy dwa kroki:\n",
    "<ol>\n",
    "    <li>Wektoryzacja - zamienia dane zapisane w postaci tekstowej na macierz z wektorami bag-of-words.</li>\n",
    "    <li>Klasyfikacja - wytrenowanie klasyfikatora.</li>\n",
    "</ol>\n",
    "W zdefiniowanym obiekcie typu pipeline, i+1 element pipeline'u na wejściu dostaje dane z wyjścia i-tego elementu (Zatem nasz klasyfikator otrzyma dane przetworzone przez TfidfVectorizer). <br/>\n",
    "Metoda fit na wejściu przyjmuje listę dokumentów w formie tekstowej, oraz oczekiwane etykiety w formie liczbowej.\n",
    "<br/>\n",
    "Analogicznie w procesie klasyfikowania nowych tekstów z użyciem istniejącego modelu - metoda predict() wykona sekwencję kroków: wektoryzacja + klasyfikacja dla zadanej listy surowych tekstów). <br/>\n",
    "Zapoznaj się z poniższym kodem i uruchom go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tekst NEED TO FIND SOMETHING? ::FREE MORTGAGE QUOTE:: To be removed from this list, click here. , zaklasyfikowany został jako: SPAM\n",
      "W zbiorze testowym 88.404% przypadków zostało poprawnie zaklasyfikowanych!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "# ------------------- WCZYTANIE DANYCH -----------\n",
    "\n",
    "# Wczytaj dane z pliku CSV\n",
    "full_dataset = pandas.read_csv('resources/spam-emails.csv', encoding='utf-8')\n",
    "# Ponieważ nazwy kategorii zapisane są z użyciem stringów: \"ham\"/\"spam\",\n",
    "# Wykonujemy mapowanie tych wartości na liczby, aby móc wykonać klasyfikację.\n",
    "full_dataset['label_num'] = full_dataset.label.map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Ustaw seed na 0, aby zapewnić powtarzalność eksperymentu\n",
    "np.random.seed(0)\n",
    "# Wylosuj 70% wierszy, które znajdą się w zbiorze treningowym\n",
    "train_indices = np.random.rand(len(full_dataset)) < 0.7\n",
    "\n",
    "# Wybierz zbiór treningowy (70%)\n",
    "train: pandas.DataFrame = full_dataset[train_indices]\n",
    "# Wybierz zbiór testowy (dopełnienie treningowego - 30%)\n",
    "test: pandas.DataFrame = full_dataset[~train_indices]\n",
    "\n",
    "# ------------------- STWORZENIE PIPELINE'U -----------\n",
    "\n",
    "# Stwórzmy pipeline surowy tekst -> TFIDF vectorizer -> klasyfikator\n",
    "pipeline = Pipeline([\n",
    "  ('tfidf', TfidfVectorizer(max_df=0.4)),\n",
    "  ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# ------------------- TRANSFORMACJA I UCZENIE -----------\n",
    "\n",
    "# Zwektoryzujmy dane i wytrenujmy klasyfikator na zbiorze treningowym\n",
    "pipeline.fit(train['text'], train['label_num'])\n",
    "\n",
    "# ------------------- KLASYFIKACJA PRZYKŁADOWEGO TEKSTU -----------\n",
    "\n",
    "text_to_predict = \"NEED TO FIND SOMETHING? ::FREE MORTGAGE QUOTE:: To be removed from this list, click here. \"\n",
    "if pipeline.predict([text_to_predict]) == 1:\n",
    "  detected = 'SPAM'\n",
    "else:\n",
    "  detected = 'HAM'\n",
    "print(f\"Tekst {text_to_predict}, zaklasyfikowany został jako: {detected}\")\n",
    "\n",
    "# ------------------- OCENA KLASYFIKATORA -----------\n",
    "accuracy = 100 * pipeline.score(test['text'], test['label_num'])\n",
    "print(f\"W zbiorze testowym {accuracy:.3f}% przypadków zostało poprawnie zaklasyfikowanych!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Zadanie 5 (1 punkt): dobór parametrów klasyfikacji\n",
    "Poniżej znajduje się kod tworzący pipeline składający się z dwóch elementów: TfidfVectorizera oraz klasyfikatora naiwnego Bayesa - MultinomialNB. Wektoryzator tworzy model bag-of-words, który uwzględnia jedynie 1000 najważniejszych słów w słowniku. W celu zastosowania stemmingu oraz lematyzatora w treningu i predykcji stoworzona została klasa TheTokenizer, która poza podziałem tekstu na słowa wykonuje również zadania normalizacji wg. ustalonych flag: **use_stemming, use_lemmatization, use_stopword_removal**. <br/>\n",
    "<strong>Zadanie 5a (0.5 punktu)</strong>: <br/>\n",
    "Zweryfikuj jak zmiana wartości flag **use_stemming, use_lemmatization, use_stopword_removal**, a co za tym idzie wykorzystanie lamatyzacji, stemmingu i usuwania najczęstszych słow wpływa na miary precision, recall i f1 stworzonego klasyfikatora. Wyniki zapisz w komentarzu. <span style=\"color: #ff0000\">(modyfikuj linie 16, 17, 18, komentarz - w kolejnej komórce)</span><br/>\n",
    "<strong> Zadanie 5b (0.5 punktu)</strong>: <br/>\n",
    "Ustaw flagi **use_stemming, use_lemmatization, use_stopword_removal** z linii 16,17 i 18 na False, i porównaj wartości precision recall i f1 dla klasyfikatora, ktory wykorzystuje CountVectorizer i takiego, który wykorzystuje TfidfVectorizer. Pozostaw parametr max_features=1000 w obu przypadkach. Który wektoryzator jest lepszy? <span style=\"color: #ff0000\">(modyfikuj linię 84)</span>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pobieranie danych...\n",
      "Pobieranie danych zakończone.\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TheTokenizer(object):\n",
    "  use_stemming: bool = False\n",
    "  use_lemmatization: bool = False\n",
    "  use_stopword_removal: bool = False\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  stemmer = PorterStemmer()\n",
    "  stopwords = set(stopwords.words('english'))\n",
    "\n",
    "  def __call__(self, doc):\n",
    "    if not self.use_stemming and not self.use_lemmatization: # tokenizuj i ew. lematyzuj/stemuj/usuń stopwords w zależności od ustawionych flag\n",
    "        return [t for t in word_tokenize(doc) if self.is_allowed(t)]\n",
    "    elif self.use_stemming and not self.use_lemmatization:\n",
    "        return [self.stem_token(t) for t in word_tokenize(doc) if self.is_allowed(t)]\n",
    "    elif self.use_lemmatization and not self.use_stemming:\n",
    "        return [self.lemmatize_token(t, pos) for t, pos in pos_tag(word_tokenize(doc)) if self.is_allowed(t)]\n",
    "\n",
    "  def stem_token(self, t):\n",
    "    return self.stemmer.stem(t)\n",
    "\n",
    "  def lemmatize_token(self, t, postag):\n",
    "    return self.lemmatizer.lemmatize(t, self.adapt_token_to_wordnet(postag))\n",
    "\n",
    "  def is_allowed(self, token: str):\n",
    "    return not self.use_stopword_removal or not token in self.stopwords\n",
    "\n",
    "  def adapt_token_to_wordnet(self, treebank_tag: str) -> str:\n",
    "    match treebank_tag:\n",
    "      case tag if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "      case tag if tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "      case tag if tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "      case _:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "print(\"Pobieranie danych...\")\n",
    "categories = [\n",
    "  'misc.forsale',\n",
    "  'soc.religion.christian',\n",
    "  'sci.space',\n",
    "  'talk.politics.guns',\n",
    "  'comp.graphics',\n",
    "  'sci.med',\n",
    "  'rec.motorcycles',\n",
    "  'sci.med',\n",
    "  'sci.electronics',\n",
    "  'talk.politics.misc',\n",
    "  'comp.sys.mac.hardware'\n",
    "]\n",
    "dataset_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "dataset_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "print(\"Pobieranie danych zakończone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Część A:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tworzenie pipeline'u - TfidfVectorizer + stemming\n",
      "Uczenie pipeline'u\n",
      "Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika)\n",
      "W słowniku znajduje się 1000 różnych słów\n",
      "Ocena klasyfikatora TfidfVectorizer + stemming:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.69      0.78      0.73       389\n",
      " comp.sys.mac.hardware       0.79      0.77      0.78       385\n",
      "          misc.forsale       0.76      0.87      0.81       390\n",
      "       rec.motorcycles       0.69      0.88      0.78       398\n",
      "       sci.electronics       0.74      0.65      0.69       393\n",
      "               sci.med       0.76      0.70      0.73       396\n",
      "             sci.space       0.89      0.75      0.82       394\n",
      "soc.religion.christian       0.82      0.89      0.86       398\n",
      "    talk.politics.guns       0.66      0.79      0.72       364\n",
      "    talk.politics.misc       0.88      0.40      0.55       310\n",
      "\n",
      "              accuracy                           0.76      3817\n",
      "             macro avg       0.77      0.75      0.75      3817\n",
      "          weighted avg       0.77      0.76      0.75      3817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tworzenie pipeline'u - TfidfVectorizer + stemming\")\n",
    "pipeline = Pipeline([\n",
    "  ('vectorizer', TfidfVectorizer(tokenizer=TheTokenizer(use_stemming=True), max_features=1000)),\n",
    "  ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "print(\"Uczenie pipeline'u\")\n",
    "pipeline.fit(dataset_train.data, dataset_train.target)\n",
    "\n",
    "print(\"Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika)\")\n",
    "print(f\"W słowniku znajduje się {len(pipeline.named_steps['vectorizer'].vocabulary_.keys())} różnych słów\")\n",
    "\n",
    "print(\"Ocena klasyfikatora TfidfVectorizer + stemming:\")\n",
    "print(\n",
    "  classification_report(\n",
    "    labels_as_strings(dataset_test.target),\n",
    "    labels_as_strings(pipeline.predict(dataset_test.data)))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tworzenie pipeline'u - TfidfVectorizer + stemming + stop_words\n",
      "Uczenie pipeline'u\n",
      "Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika)\n",
      "W słowniku znajduje się 1000 różnych słów\n",
      "Ocena klasyfikatora TfidfVectorizer + stemming + stop_words:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.68      0.79      0.73       389\n",
      " comp.sys.mac.hardware       0.79      0.76      0.77       385\n",
      "          misc.forsale       0.76      0.87      0.81       390\n",
      "       rec.motorcycles       0.70      0.90      0.79       398\n",
      "       sci.electronics       0.75      0.65      0.70       393\n",
      "               sci.med       0.80      0.72      0.76       396\n",
      "             sci.space       0.90      0.79      0.84       394\n",
      "soc.religion.christian       0.88      0.91      0.90       398\n",
      "    talk.politics.guns       0.68      0.81      0.74       364\n",
      "    talk.politics.misc       0.86      0.43      0.57       310\n",
      "\n",
      "              accuracy                           0.77      3817\n",
      "             macro avg       0.78      0.76      0.76      3817\n",
      "          weighted avg       0.78      0.77      0.77      3817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tworzenie pipeline'u - TfidfVectorizer + stemming + stop_words\")\n",
    "pipeline = Pipeline([\n",
    "  ('vectorizer',\n",
    "   TfidfVectorizer(\n",
    "     tokenizer=TheTokenizer(use_stemming=True, use_stopword_removal=True),\n",
    "     max_features=1000)\n",
    "   ),\n",
    "  ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "print(\"Uczenie pipeline'u\")\n",
    "pipeline.fit(dataset_train.data, dataset_train.target)\n",
    "\n",
    "print(\"Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika)\")\n",
    "print(f\"W słowniku znajduje się {len(pipeline.named_steps['vectorizer'].vocabulary_.keys())} różnych słów\")\n",
    "\n",
    "print(\"Ocena klasyfikatora TfidfVectorizer + stemming + stop_words:\")\n",
    "print(\n",
    "  classification_report(\n",
    "    labels_as_strings(dataset_test.target),\n",
    "    labels_as_strings(pipeline.predict(dataset_test.data)))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tworzenie pipeline'u - TfidfVectorizer + lemmatization\n",
      "Uczenie pipeline'u\n",
      "Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika)\n",
      "W słowniku znajduje się 1000 różnych słów\n",
      "Ocena klasyfikatora TfidfVectorizer + lemantization:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.70      0.77      0.73       389\n",
      " comp.sys.mac.hardware       0.79      0.76      0.77       385\n",
      "          misc.forsale       0.77      0.87      0.82       390\n",
      "       rec.motorcycles       0.68      0.88      0.77       398\n",
      "       sci.electronics       0.73      0.65      0.69       393\n",
      "               sci.med       0.78      0.70      0.74       396\n",
      "             sci.space       0.88      0.77      0.82       394\n",
      "soc.religion.christian       0.83      0.91      0.87       398\n",
      "    talk.politics.guns       0.67      0.78      0.72       364\n",
      "    talk.politics.misc       0.83      0.38      0.52       310\n",
      "\n",
      "              accuracy                           0.76      3817\n",
      "             macro avg       0.77      0.75      0.75      3817\n",
      "          weighted avg       0.76      0.76      0.75      3817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tworzenie pipeline'u - TfidfVectorizer + lemmatization\")\n",
    "pipeline = Pipeline([\n",
    "  ('vectorizer', TfidfVectorizer(\n",
    "    tokenizer=TheTokenizer(use_lemmatization=True),\n",
    "    max_features=1000)\n",
    "   ),\n",
    "  ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "print(\"Uczenie pipeline'u\")\n",
    "pipeline.fit(dataset_train.data, dataset_train.target)\n",
    "\n",
    "print(\"Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika)\")\n",
    "print(f\"W słowniku znajduje się {len(pipeline.named_steps['vectorizer'].vocabulary_.keys())} różnych słów\")\n",
    "\n",
    "print(\"Ocena klasyfikatora TfidfVectorizer + lemantization:\")\n",
    "print(\n",
    "  classification_report(\n",
    "    labels_as_strings(dataset_test.target),\n",
    "    labels_as_strings(pipeline.predict(dataset_test.data)))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.70      0.68      0.69       389\n",
      " comp.sys.mac.hardware       0.67      0.72      0.70       385\n",
      "          misc.forsale       0.76      0.84      0.80       390\n",
      "       rec.motorcycles       0.73      0.86      0.79       398\n",
      "       sci.electronics       0.65      0.67      0.66       393\n",
      "               sci.med       0.81      0.63      0.71       396\n",
      "             sci.space       0.85      0.75      0.80       394\n",
      "soc.religion.christian       0.90      0.88      0.89       398\n",
      "    talk.politics.guns       0.61      0.63      0.62       364\n",
      "    talk.politics.misc       0.58      0.55      0.56       310\n",
      "\n",
      "              accuracy                           0.73      3817\n",
      "             macro avg       0.73      0.72      0.72      3817\n",
      "          weighted avg       0.73      0.73      0.73      3817\n",
      "\n",
      "Tworzenie pipeline'u - TfidfVectorizer + lemmatization + stop_words\n",
      "Uczenie pipeline'u\n",
      "Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika)\n",
      "W słowniku znajduje się 1000 różnych słów\n",
      "Ocena klasyfikatora TfidfVectorizer + lemmantization + stop_words:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [182]\u001B[0m, in \u001B[0;36m<cell line: 18>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mW słowniku znajduje się \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(pipeline\u001B[38;5;241m.\u001B[39mnamed_steps[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvectorizer\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvocabulary_\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m różnych słów\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOcena klasyfikatora TfidfVectorizer + lemmantization + stop_words:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m     19\u001B[0m   classification_report(\n\u001B[0;32m     20\u001B[0m     labels_as_strings(dataset_test\u001B[38;5;241m.\u001B[39mtarget),\n\u001B[1;32m---> 21\u001B[0m     labels_as_strings(\u001B[43mpipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_test\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[0;32m     22\u001B[0m )\n",
      "File \u001B[1;32mc:\\users\\hououin kyouma\\pycharmprojects\\nlp-laboratories\\venv\\lib\\site-packages\\sklearn\\utils\\metaestimators.py:113\u001B[0m, in \u001B[0;36m_AvailableIfDescriptor.__get__.<locals>.<lambda>\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    110\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m attr_err\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;66;03m# lambda, but not partial, allows help() to work with update_wrapper\u001B[39;00m\n\u001B[1;32m--> 113\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(obj, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfn\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
      "File \u001B[1;32mc:\\users\\hououin kyouma\\pycharmprojects\\nlp-laboratories\\venv\\lib\\site-packages\\sklearn\\pipeline.py:469\u001B[0m, in \u001B[0;36mPipeline.predict\u001B[1;34m(self, X, **predict_params)\u001B[0m\n\u001B[0;32m    467\u001B[0m Xt \u001B[38;5;241m=\u001B[39m X\n\u001B[0;32m    468\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, name, transform \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter(with_final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m--> 469\u001B[0m     Xt \u001B[38;5;241m=\u001B[39m \u001B[43mtransform\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    470\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mpredict(Xt, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpredict_params)\n",
      "File \u001B[1;32mc:\\users\\hououin kyouma\\pycharmprojects\\nlp-laboratories\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2101\u001B[0m, in \u001B[0;36mTfidfVectorizer.transform\u001B[1;34m(self, raw_documents)\u001B[0m\n\u001B[0;32m   2084\u001B[0m \u001B[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001B[39;00m\n\u001B[0;32m   2085\u001B[0m \n\u001B[0;32m   2086\u001B[0m \u001B[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2097\u001B[0m \u001B[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001B[39;00m\n\u001B[0;32m   2098\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2099\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m, msg\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe TF-IDF vectorizer is not fitted\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2101\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mtransform(X, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32mc:\\users\\hououin kyouma\\pycharmprojects\\nlp-laboratories\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1379\u001B[0m, in \u001B[0;36mCountVectorizer.transform\u001B[1;34m(self, raw_documents)\u001B[0m\n\u001B[0;32m   1376\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_vocabulary()\n\u001B[0;32m   1378\u001B[0m \u001B[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001B[39;00m\n\u001B[1;32m-> 1379\u001B[0m _, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfixed_vocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   1380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[0;32m   1381\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mc:\\users\\hououin kyouma\\pycharmprojects\\nlp-laboratories\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1199\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m raw_documents:\n\u001B[0;32m   1200\u001B[0m     feature_counter \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m-> 1201\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m \u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1202\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1203\u001B[0m             feature_idx \u001B[38;5;241m=\u001B[39m vocabulary[feature]\n",
      "File \u001B[1;32mc:\\users\\hououin kyouma\\pycharmprojects\\nlp-laboratories\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:115\u001B[0m, in \u001B[0;36m_analyze\u001B[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[0;32m    113\u001B[0m     doc \u001B[38;5;241m=\u001B[39m preprocessor(doc)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 115\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ngrams \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stop_words \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "Input \u001B[1;32mIn [176]\u001B[0m, in \u001B[0;36mTheTokenizer.__call__\u001B[1;34m(self, doc)\u001B[0m\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstem_token(t) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m word_tokenize(doc) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_allowed(t)]\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_lemmatization \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_stemming:\n\u001B[1;32m---> 29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlemmatize_token(t, pos) \u001B[38;5;28;01mfor\u001B[39;00m t, pos \u001B[38;5;129;01min\u001B[39;00m \u001B[43mpos_tag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_allowed(t)]\n",
      "File \u001B[1;32mc:\\users\\hououin kyouma\\pycharmprojects\\nlp-laboratories\\venv\\lib\\site-packages\\nltk\\tag\\__init__.py:166\u001B[0m, in \u001B[0;36mpos_tag\u001B[1;34m(tokens, tagset, lang)\u001B[0m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;124;03mtag the given list of tokens.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;124;03m:rtype: list(tuple(str, str))\u001B[39;00m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    165\u001B[0m tagger \u001B[38;5;241m=\u001B[39m _get_tagger(lang)\n\u001B[1;32m--> 166\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_pos_tag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtagset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtagger\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlang\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\hououin kyouma\\pycharmprojects\\nlp-laboratories\\venv\\lib\\site-packages\\nltk\\tag\\__init__.py:123\u001B[0m, in \u001B[0;36m_pos_tag\u001B[1;34m(tokens, tagset, tagger, lang)\u001B[0m\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokens: expected a list of strings, got a string\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 123\u001B[0m     tagged_tokens \u001B[38;5;241m=\u001B[39m \u001B[43mtagger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tagset:  \u001B[38;5;66;03m# Maps to the specified tagset.\u001B[39;00m\n\u001B[0;32m    125\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m lang \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meng\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32mc:\\users\\hououin kyouma\\pycharmprojects\\nlp-laboratories\\venv\\lib\\site-packages\\nltk\\tag\\perceptron.py:187\u001B[0m, in \u001B[0;36mPerceptronTagger.tag\u001B[1;34m(self, tokens, return_conf, use_tagdict)\u001B[0m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tag:\n\u001B[0;32m    186\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_features(i, word, context, prev, prev2)\n\u001B[1;32m--> 187\u001B[0m     tag, conf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_conf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    188\u001B[0m output\u001B[38;5;241m.\u001B[39mappend((word, tag, conf) \u001B[38;5;28;01mif\u001B[39;00m return_conf \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m (word, tag))\n\u001B[0;32m    190\u001B[0m prev2 \u001B[38;5;241m=\u001B[39m prev\n",
      "File \u001B[1;32mc:\\users\\hououin kyouma\\pycharmprojects\\nlp-laboratories\\venv\\lib\\site-packages\\nltk\\tag\\perceptron.py:66\u001B[0m, in \u001B[0;36mAveragedPerceptron.predict\u001B[1;34m(self, features, return_conf)\u001B[0m\n\u001B[0;32m     64\u001B[0m     weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights[feat]\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m label, weight \u001B[38;5;129;01min\u001B[39;00m weights\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m---> 66\u001B[0m         scores[label] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m value \u001B[38;5;241m*\u001B[39m weight\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# Do a secondary alphabetic sort, for stability\u001B[39;00m\n\u001B[0;32m     69\u001B[0m best_label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses, key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m label: (scores[label], label))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Tworzenie pipeline'u - TfidfVectorizer + lemmatization + stop_words\")\n",
    "pipeline = Pipeline([\n",
    "  ('vectorizer',\n",
    "   TfidfVectorizer(\n",
    "     tokenizer=TheTokenizer(use_lemmatization=True, use_stopword_removal=True),\n",
    "     max_features=1000)\n",
    "   ),\n",
    "  ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "print(\"Uczenie pipeline'u\")\n",
    "pipeline.fit(dataset_train.data, dataset_train.target)\n",
    "\n",
    "print(\"Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika)\")\n",
    "print(f\"W słowniku znajduje się {len(pipeline.named_steps['vectorizer'].vocabulary_.keys())} różnych słów\")\n",
    "\n",
    "print(\"Ocena klasyfikatora TfidfVectorizer + lemmantization + stop_words:\")\n",
    "print(\n",
    "  classification_report(\n",
    "    labels_as_strings(dataset_test.target),\n",
    "    labels_as_strings(pipeline.predict(dataset_test.data)))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wnioski: [ Precision | recall | f1-score | supprt ]\n",
    " - bez niczego -- 0.74 0.72 0.71 3817\n",
    " - same stemowanie -- 0.77 0.75 0.75 3817\n",
    " - stemowanie ze stop_words -- 0.78 0.76 0.76 3817\n",
    " - sama lematyzacja -- 0.77 0.75 0.75 3817\n",
    " - lematyzacja ze stop_words -- 0.73 0.72 0.72 3817"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Część B:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tworzenie pipeline'u - CountVectorizer\n",
      "Uczenie pipeline'u\n",
      "Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika)\n",
      "W słowniku znajduje się 1000 różnych słów\n",
      "Ocena klasyfikatora CountVectorizer:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.67      0.66      0.66       389\n",
      " comp.sys.mac.hardware       0.62      0.66      0.64       385\n",
      "          misc.forsale       0.72      0.86      0.78       390\n",
      "       rec.motorcycles       0.61      0.81      0.69       398\n",
      "       sci.electronics       0.61      0.58      0.59       393\n",
      "               sci.med       0.78      0.54      0.64       396\n",
      "             sci.space       0.76      0.69      0.72       394\n",
      "soc.religion.christian       0.88      0.83      0.86       398\n",
      "    talk.politics.guns       0.58      0.49      0.53       364\n",
      "    talk.politics.misc       0.49      0.54      0.51       310\n",
      "\n",
      "              accuracy                           0.67      3817\n",
      "             macro avg       0.67      0.67      0.66      3817\n",
      "          weighted avg       0.68      0.67      0.67      3817\n",
      "\n",
      "Tworzenie pipeline'u - TfidfVectorizer\n",
      "Uczenie pipeline'u\n",
      "Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika)\n",
      "W słowniku znajduje się 1000 różnych słów\n",
      "Ocena klasyfikatora - TfidfVectorizer:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.66      0.75      0.70       389\n",
      " comp.sys.mac.hardware       0.76      0.69      0.73       385\n",
      "          misc.forsale       0.76      0.88      0.82       390\n",
      "       rec.motorcycles       0.61      0.88      0.72       398\n",
      "       sci.electronics       0.69      0.54      0.61       393\n",
      "               sci.med       0.73      0.65      0.69       396\n",
      "             sci.space       0.86      0.75      0.80       394\n",
      "soc.religion.christian       0.81      0.89      0.85       398\n",
      "    talk.politics.guns       0.65      0.76      0.70       364\n",
      "    talk.politics.misc       0.84      0.36      0.51       310\n",
      "\n",
      "              accuracy                           0.72      3817\n",
      "             macro avg       0.74      0.72      0.71      3817\n",
      "          weighted avg       0.74      0.72      0.72      3817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tworzenie pipeline'u - CountVectorizer\")\n",
    "pipeline = Pipeline([\n",
    "  ('vectorizer', CountVectorizer(tokenizer=TheTokenizer(), max_features=1000)),\n",
    "  ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "print(\"Uczenie pipeline'u\")\n",
    "pipeline.fit(dataset_train.data, dataset_train.target)\n",
    "\n",
    "print(\"Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika)\")\n",
    "print(f\"W słowniku znajduje się {len(pipeline.named_steps['vectorizer'].vocabulary_.keys())} różnych słów\")\n",
    "\n",
    "print(\"Ocena klasyfikatora CountVectorizer:\")\n",
    "print(\n",
    "  classification_report(\n",
    "    labels_as_strings(dataset_test.target),\n",
    "    labels_as_strings(pipeline.predict(dataset_test.data)))\n",
    ")\n",
    "\n",
    "print(\"Tworzenie pipeline'u - TfidfVectorizer\")\n",
    "pipeline = Pipeline([\n",
    "  ('vectorizer', TfidfVectorizer(tokenizer=TheTokenizer(), max_features=1000)),\n",
    "  ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "print(\"Uczenie pipeline'u\")\n",
    "pipeline.fit(dataset_train.data, dataset_train.target)\n",
    "\n",
    "print(\"Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika)\")\n",
    "print(f\"W słowniku znajduje się {len(pipeline.named_steps['vectorizer'].vocabulary_.keys())} różnych słów\")\n",
    "\n",
    "print(\"Ocena klasyfikatora - TfidfVectorizer:\")\n",
    "print(\n",
    "  classification_report(\n",
    "    labels_as_strings(dataset_test.target),\n",
    "    labels_as_strings(pipeline.predict(dataset_test.data)))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lepsze wyniki uzyskuje wektoryzator oparty o Tfid,\n",
    "pierwszy otrzymał średnią precyzję\n",
    " - CountVectorizer - 67%,\n",
    " - TfidVectorizer - 74%."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}