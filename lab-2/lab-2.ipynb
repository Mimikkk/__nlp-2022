{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytywanie danych\n",
    "Paczka Sklearn (Scikit Learn) to kolejne bardzo popularne narzędzie do uczenia maszynowego. Posiada bardzo przejrzyste API i spore wsparcie (scikit-learn.org/). W dzisiejszych zadaniach skupimy się na ładowaniu zbiorów danych, ich transformacji oraz algorytmach klasyfikacji. Podczas dzisiejszych laboratoriów wykorzystamy:\n",
    "<ul>\n",
    "    <li>NLTK - udostępniające metody prostego przetwarzania tekstu (tokenizacja, lematyzacja, stemming)</li>\n",
    "    <li>Sklearn - paczkę do uczenia maszynowego</li>\n",
    "    <li>Pandas - bibliotekę do wczytywania i obsługi zbiorów danych</li>\n",
    "</ul>\n",
    "<span style=\"color: #ff0000\">Ponieważ część kodu jest już stworzona, w każdym zadaniu wyszczególnione są numery linii, w których należy wprowadzić modyfikacje, aby rozwiązać zadanie. Jeśli nie widzisz numeracji linii w kodzie w otwartym notebooku - możesz włączyć tę funkcjonalność poprzez wybór View -> toggle line numbers w górnym menu.</span><br/><br/>\n",
    "Najpierw wczytajmy dane tekstowe ze zbioru, w którym posiadamy zestaw wiadomości e-mail oznaczonych jako spamowe lub niespamowe.\n",
    "Ponieważ będziemy rozwiązywać problem klasyfikacji, oddzielamy dane do trenowania klasyfikatora oraz do weryfikacji jego jakości.\n",
    "<br/>\n",
    "\n",
    "<strong>Przeanalizuj i uruchom poniższy fragment kodu.</strong> Załaduje on odpowiednie dane do dwóch obiektów:\n",
    "<ol>\n",
    "<li>train: zbiór treningowy - dokumenty na których nauczymy klasyfikator</li>\n",
    "<li>test: zbiór testowy - dokumenty na których przetestujemy klasyfikator</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- Ładowanie danych i oddzielanie zbioru treningowego od testowego ------\n",
    "full_dataset = pandas.read_csv('resources/spam-emails.csv', encoding='utf-8')\n",
    "\n",
    "# ponieważ nazwy kategorii zapisane są z użyciem stringów: \"ham\"/\"spam\", wykonujemy mapowanie tych wartości na liczby, co będzie potrzebne do wykonania klasyfikacji.\n",
    "full_dataset['label_num'] = full_dataset.label.map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# ustaw seed na 0, aby zapewnić powtarzalność eksperymentu\n",
    "np.random.seed(0)\n",
    "train_indices = np.random.rand(len(full_dataset)) < 0.7\n",
    "train = full_dataset[train_indices]\n",
    "test = full_dataset[~train_indices]\n",
    "\n",
    "# ---------------- Wyświetlanie statystyk -----------------\n",
    "print(f\"Elementów w zbiorze treningowym/testowym: {len(train)}/{len(test)}\")\n",
    "print(f\"Liczność klas w zbiorze treningowym/testowym: {train.label.value_counts()}/{test.label.value_counts()}\")\n",
    "\n",
    "# wyświetl próbkę danych\n",
    "full_dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformacja danych\n",
    "Aby zastosować większość algorytmów uczenia maszynowego - dane wejściowe muszą być reprezentowane jako wektory liczb. Wykorzystajmy zatem narzędzia dostarczone przez Scikit-learn do tego celu. Użyjmy klasy **CountVectorizer()**, aby podzielić poszczególne dokumenty na słowa, a następnie stworzyć reprezentację \"bag of words\"\n",
    "Dla przypomnienia - \"bag of words\" tworzony jest w nastepujący sposób:\n",
    "<ol>\n",
    "<li>Przeglądamy wszystkie dostępne dokumenty i tworzymy listę wszystkich unikalnych słów jakie napotkaliśmy (słownik).</li>\n",
    "<li>Stworzona lista wyznacza nam wektor cech - każda pozycja w takim wektorze oznacza jedno z napotkanych słów.</li>\n",
    "<li>Każdy z dokumentów mapowany jest na wektor cech poprzez zapisanie ile razy każde ze słów dokumentu wystąpiło w nim.</li>\n",
    "</ol>\n",
    "Przykład wektoryzacji znajduje się poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PRZYKŁAD ------------------------------------------\n",
    "# np. Dla dwóch dokumentów:\n",
    "# Dokument 1: Ala ma kota i ma psa \n",
    "# Dokument 2: Kot ma Alę\n",
    "\n",
    "# Poszczególne kroki wyglądają następująco:\n",
    "# Lista unikalnych słów:                 [Ala, ma, kota, i, psa, Kot, Alę] \n",
    "# Szablon wektora cech:                  [  0,  0,    0, 0,   0,   0,   0] - wektor jest tyluelementowy, ile mamy  unikalnych słów \n",
    "# Osadzenie dokumenu 1 jako wektor cech: [  1,  2,    1, 1,   1,   0,   0] - słowo \"ma\" pojawia się w dok. 2 razy, \"kot\" i \"Alę\" - wcale\n",
    "# Analogicznie dokument 2:               [  0,  1,    0, 0,   0,   1,   1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "# stwórz macierz liczbową z danych. W wierszach mamy kolejne dokumenty, w kolumnach kolejne pola wektora cech odpowiadające unikalnym słowom (bag of words)\n",
    "X_train_counts = vectorizer.fit_transform(train['text'])\n",
    "# analogicznie jak wyżej - dla zbioru testowego.\n",
    "X_test_counts = vectorizer.transform(test['text'])\n",
    "\n",
    "# wyświetl rozmiar macierzy. Pierwsze pole - liczba dokumentów, drugie - liczba cech (stała dla wszystkich dokumentów)\n",
    "print(f\"Rozmiar stworzonej macierzy: {X_train_counts.shape}\")\n",
    "print(f\"Liczba dokumentów: {X_train_counts.shape[0]}\")\n",
    "print(f\"Rozmiar wektora bag-of-words {X_train_counts.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uwaga:\n",
    "Na zbiorze treningowym użyto funkcji - fit_transform(), na testowym - transform(). <br/>\n",
    "**Dlaczego?** fit_transform() wykonuje dwie operacje - tworzy i zapisuje listę wszystkich unikalnych słów (słownik) oraz zamienia dokument na wektor o długości takiej jak słownik. transform() natomiast wykorzystuje istniejący już słownik i wykonuje z jego użyciem transformację do wektora. \n",
    "<br/>\n",
    "Ponieważ zbiór treningowy jest zazwyczaj liczniejszy - z reguły znajdziemy w nim więcej różnych słów. Ponadto, wszystkie słowa, które mogą pomóc w klasyfikacji i tak muszą znaleźć się w zbiorze treningowym aby móc się nauczyć ich wykorzystania. Nie nadpisuje się zatem słownika za pomocą zbioru testowego i tworzy się go tylko raz - podczas treningu, wykorzystując go następnie do tworzenia nowych wektorów z nieobserwowanych podczas treningu dokumentów.\n",
    "\n",
    "# Zadanie 1 (1 punkt):\n",
    "Jak się pewnie domyślasz - reprezentacja bag-of-words będzie miała bardzo wiele zer w wygenerowanych macierzach (macierzach, w których w poszczególnych wierszach będziemy mieli poszczególne dokumenty, a w kolumnach wektory słów reprezentacji bag of words). Rozmiar macierzy z poprzedniego listingu pokazuje, że każdy dokument opisany jest wektorem 37325 pozycji, ponieważ tyle różnych słów zostało wykrytych po analizie wszystkich dokumentów treningowych. Większość dokumentów analizowanych osobbno zawierać będzie pewnie co najwyżej kilkadziesiąt/kilkaset różnych słów.\n",
    "\n",
    "***Zadanie: Napisz fragment kodu, który zliczy:***\n",
    "<ol>\n",
    "    <li><strong>jaki procent macierzy X_train_counts ma elementy o wartości różnej od zera</strong></li>\n",
    "    <li><strong>ile tokenów (łącznie, nie tylko unikalne) występuje w macierzy X_train_counts?</strong></li>\n",
    "</ol>\n",
    "Wskazówka - ponieważ zer w tej macierzy jest istotnie dużo - dane po transformacji CountVectorizerem trzymane są w specjalnym formacie, w którym zapisuje się tylko elementy mające wartości różne od zera, w tzw. macierzy rzadkiej (sparse matrix). Aby przeiterować po takiej macierzy, można wykorzystać następujące fragmenty kodu:\n",
    "\n",
    "<strong>cx = X_train_counts.tocoo()</strong> - transformuj macierz do reprezentacji koordynatowej (patrz niżej) <br/>\n",
    "<strong>for doc_id, word_id, count in zip(cx.row, cx.col, cx.data):</strong> - pozwala ona na iterowanie po wszystkich niezerowych elementach, w każdym kroku otrzymując 3 zmienne - numer wiersza (numer dokumentu), numer kolumny (identyfikator słowa ze słownika) oraz licznik mówiący ile razy dane słowo wystąpiło w danym dokumencie. <span style=\"color: #ff0000\">(Do wykonania zadania musisz zaktualizować linijki 3, 7 i 8)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# tu zapisz ilość komórek w macierzy (ilość wierszy * ilość kolumn, rozważ użycie pola 'shape' na macierzy X_train_counts)\u001B[39;00m\n\u001B[0;32m      6\u001B[0m count_all \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m  \n\u001B[1;32m----> 8\u001B[0m cx \u001B[38;5;241m=\u001B[39m \u001B[43mX_train_counts\u001B[49m\u001B[38;5;241m.\u001B[39mtocoo()\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m doc_id, word_id, count \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(cx\u001B[38;5;241m.\u001B[39mrow, cx\u001B[38;5;241m.\u001B[39mcol, cx\u001B[38;5;241m.\u001B[39mdata):  \u001B[38;5;66;03m#iteracja po elementach niezerowych\u001B[39;00m\n\u001B[0;32m     10\u001B[0m   count_tokens \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X_train_counts' is not defined"
     ]
    }
   ],
   "source": [
    "# tu zapisz liczbę wszystkich tokenów w macierzy\n",
    "count_tokens = 0\n",
    "# tu zapisz ilość elementów niezerowych w macierzy\n",
    "count_nonzero = 0\n",
    "# tu zapisz ilość komórek w macierzy (ilość wierszy * ilość kolumn, rozważ użycie pola 'shape' na macierzy X_train_counts)\n",
    "count_all = 0\n",
    "\n",
    "cx = X_train_counts.tocoo()\n",
    "for doc_id, word_id, count in zip(cx.row, cx.col, cx.data):  #iteracja po elementach niezerowych\n",
    "  count_tokens += 0\n",
    "  count_nonzero += 0\n",
    "\n",
    "print(f\"W datasecie znajduje się: {count_tokens} tokenów. Macierz posiada {100.0 * count_nonzero / count_all:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <strong>Oczekiwany rezultat:</strong> <br/>\n",
    "Mniej niż 1% elementów niezerowych (!) <br/>\n",
    "Ponad 400000 tokenów\n",
    "</div>\n",
    "\n",
    "# Zadanie 2 (1 punkt) - słowa charakteryzujące klasy\n",
    "\n",
    "Wykorzystajmy macierz X_train_counts wykorzystywaną w poprzednim zadaniu, a także etykiety kategorii, aby stworzyć listy najczęściej występuących słów w danych kategoriach. <br/><br/>\n",
    "Aby ułatwić zadanie, utworzono większość funkcji **get_top_occuring_words()** tworzącej taki ranking<br/>\n",
    "**Zadanie 2a (0.5 punktu)**: twoim zadaniem jest zaktualizowanie wartości pola: **category_word_counts[category][word]**, tak, aby poprawnie zliczyć ile razy dane słowo wystąpiło w kategorii. <span style=\"color: #ff0000\">(zaktualizuj linijkę 20)</span>\n",
    "<br/>\n",
    "Czy najczęstsze słowa pozwalają rozdzielić kategorie SPAM od HAM?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def get_top_occuring_words(X_train_counts, how_many_words, vectorizer, train):\n",
    "  # stwórz mapowanie pozycji wektora bag-of-words na konkretne słowa\n",
    "  id_to_word = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "  cx = X_train_counts.tocoo()\n",
    "\n",
    "  # słownik, w którym przeprowadzimy zliczanie\n",
    "  category_word_counts = dict()\n",
    "\n",
    "  for doc_id, word_id, count in zip(cx.row, cx.col, cx.data):\n",
    "    # w category znajduje się idetyfikator kategorii dla aktualnego dokumentu, zapisujemy go\n",
    "    category = train.iloc[doc_id]['label']\n",
    "    # w word - aktualne słowo z dokumentu\n",
    "    # mamy też liczność wystąpienia danego słowa w dokumencie (gdzie? :) )\n",
    "    word = id_to_word[word_id]\n",
    "\n",
    "    # stwórzmy słownik z kategoriami jako kluczami\n",
    "    if category not in category_word_counts.keys():\n",
    "      # jeśli widzimy nową kategorię - dodajemy do słownika\n",
    "      category_word_counts[category] = dict()\n",
    "\n",
    "    # w ramach każdej kategorii będziemy zliaczać słowa\n",
    "    if word not in category_word_counts[category]:\n",
    "      # jeśli aktualne słowo jeszce nie zotało uwzględnione w kategorii - zainicjujmy jego licznik liczbą 0\n",
    "      category_word_counts[category][word] = 0.0\n",
    "\n",
    "    category_word_counts[category][word] += 0\n",
    "\n",
    "  # wyświetl nazwy kategorii i n najczęściej występujących w nich słów\n",
    "  for category_name in category_word_counts.keys():\n",
    "    # posortowany dict() słowo -> liczność, wg liczności, malejąco\n",
    "    sorted_cat = sorted(category_word_counts[category_name].items(), key=operator.itemgetter(1), reverse=True)\n",
    "    # wyświetl nazwę kategorii i top n słów\n",
    "    print(f\"{category_name}: {[word for word, count in sorted_cat[:how_many_words]]}\")\n",
    "\n",
    "\n",
    "# wywołanie funkcji\n",
    "get_top_occuring_words(X_train_counts, 12, vectorizer, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wektoryzacja Tf-Idf\n",
    "\n",
    "Po wykonaniu zadania 2a widzimy, że najczęściej występujące słowa w każdej kategorii mają niewielką użytezczność (pasują do każdej kategorii). Aby sprawić, żeby na czele rankingu znalazły się słowa charakterystyczne dla danej klasy, możemy użyć metody Tf-idf. <br/>\n",
    "**Zadanie 2b:\n",
    "Nadpisz wartości X_train_counts oraz X_test_counts wykorzystując w tym celu TfidfVectorizer** (http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) zamiast CountVectorizer, ustaw parametr max_df na 0.4 (tzn. ignoruj słowa, które występują w więcej niż 40% dokumentów). Następnie wykonaj stworzoną w zadaniu 2 funkcję get_top_occuring_words(), aby sprawdzić, czy ranking najważniejszych słów się zmienił. Czy zmieniony zestaw słów lepiej reprezentuje kategorie? <span style=\"color: #ff0000\">(zaktualizuj linie 3, 4, 5)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = None\n",
    "# stwórz macierz wektorów. W wierszach mamy kolejne dokumenty, w kolumnach kolejne pola wektora cech odpowiadające unikalnym słowom\n",
    "X_train_counts = None\n",
    "# analogicznie dla zbioru testowego.\n",
    "X_test_counts = None\n",
    "\n",
    "# wywołanie funkcji\n",
    "get_top_occuring_words(X_train_counts, 12, vectorizer, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 3 - Stemming i lematyzacja (1 punkt)\n",
    "Często istotne słowa występują w wielu odmianach (szczególnie w językach fleksyjnych, takich jak nasz), np: university - universities ; pay - paid - paying - pays . Wielość odmian słów ma swoje przełożenie na rozmiar słownika.\n",
    "<br/><br/>\n",
    "W niektórych warunkach, w szczególności:\n",
    "<ul>\n",
    "<li>Kiedy mamy ograniczoną pamięć</li>\n",
    "<li>Kiedy ważny jest dla nas czas działania algorytmu</li>\n",
    "<li>Kiedy istnieje ryzyko przeuczenia</li>\n",
    "</ul>\n",
    "warto rozważyć znormalizowanie słów, tak, aby zmniejszyć rozmiar słownika, a co za tym idzie wymagania pamięciowe (a co za tym idzie - czas treningu/klasyfikacji). Ograniczenie rozmiaru słownika może też zapobiec przeuczeniu. Normalizację możemy wykonać np. poprzez zastosowanie stemmingu lub lematyzacji dla poszczególnych wyrazów.\n",
    "<br/>\n",
    "<strong>Zadanie 3a (0.5 punktu)</strong>: Z użyciem biblioteki NLTK wykonaj zarówno lematyzację (używając WordNetLemmatizer) jak i stemming (używając PorterStemmer) tekstu zawartego w sample_text. Uwaga - lematyzator opcjonalnie wymaga pos-tagu dla tokenu. Przekaż do funkcji lematyzującej zmienną current_word_postag jako drugi argument. <span style=\"color: #ff0000\">(zaktualizuj linie 20, 21, 34, 35)</span>\n",
    "\n",
    "<strong>Zadanie 3b (0.5 punktu)</strong>: O ile zmniejszyła się liczba unikalnych słów po zastosowaniu lematyzacji? Odpowiedź zawrzyj w komentarzu. <span style=\"color: #ff0000\">(linijki 43:45)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# lematyzator wymaga, aby dla danego słowa podać mu, czy jest to czasownik, rzeczownik czy inny POS-tag.\n",
    "# Funkcja jest adapterem tagów nadanych przez funkcję pos_tag do tagów wymaganych przez lematyzator\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "  if treebank_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif treebank_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif treebank_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif treebank_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:\n",
    "    # As default pos in lemmatization is Noun\n",
    "    return wordnet.NOUN\n",
    "\n",
    "wordnet_lemmatizer = None\n",
    "porter_stemmer = None\n",
    "\n",
    "sample_text = \"There are some cheaper alternatives for buying the red trousers. There is a discount, it is so cheap!\"\n",
    "\n",
    "# tutaj będziemy dopisywać zlematyzowane słowa\n",
    "lemmatized = []\n",
    "# tutaj będziemy dopisywać wystemowane słowa\n",
    "stemmed = []\n",
    "\n",
    "# dzielimy tekst na słowa\n",
    "tokenized = word_tokenize(sample_text)\n",
    "# nadajemy pos-tagi (rzeczownik, czasownik przymiotnik...) każdemu słowu\n",
    "pos_tokens = nltk.pos_tag(tokenized)\n",
    "\n",
    "# dla każdego słowa\n",
    "for i in range(len(tokenized)):\n",
    "  # pobieramy pos-tag słowa\n",
    "  current_word_postag = get_wordnet_pos(pos_tokens[i][1])\n",
    "  lemmatized_token = ''\n",
    "  stemmed_token = ''\n",
    "\n",
    "  lemmatized.append(lemmatized_token)\n",
    "  stemmed.append(stemmed_token)\n",
    "\n",
    "print(f\"Bazowy tekst:        {sample_text}\")\n",
    "print(f\"Wystemowany tekst:   {' '.join(stemmed)}\")\n",
    "print(f\"Zlematyzowany tekst: {' '.join(lemmatized)}\")\n",
    "\n",
    "# Ile uniklanych tokenów znajduje się w tekście bazowym?:\n",
    "# Ile unikalnych tokenów znajduje się w tekście wystemowanym?\n",
    "# Ile unikalnych tokenów w tekście zlematyzowanym?:\n",
    "# Różnica ilości unikalnych tokenów między tekstem bazowym a wystemowanym i bazowym a zlematyzowanym:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Zadanie 4 (1 punkt) - klasyfikacja i interpretacja wyników\n",
    "Mając już dobrą reprezentację danych i wiedząc jak działa normalizacja - możemy klasyfikować! <br/>\n",
    "Istnieje wiele algorytmów, które dobrze radzą sobie z klasyfikacją tekstu, kilka przykładów to: \n",
    "<ul>\n",
    "<li>Naiwny klasyfikator Bayesa</li>\n",
    "<li>Maszyna wektorów nośnych - SVM</li>\n",
    "<li>Sieci neuronowe</li>\n",
    "</ul>\n",
    "O sieciach neuronowych więcej powiemy na jednych z przyszłych laboratoriów. <br/>\n",
    "<strong>Zadanie 4a (0.5 punktu)</strong> Wykorzystując przetworzoną postać danych: X_train_counts, X_test_counts z poprzednich zadań oraz dokumentację sklearn, zaimplementuj klasyfikację z użyciem naiwnego klasyfikatora Bayesa (MultinomialNB). <span style=\"color: #ff0000\">(zaktualizuj linie 7, 9, 12)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# funkcja pomocnicza zamieniająca identyfikatory numeryczne na tekstowe\n",
    "def labels_as_strings(vector_of_indices):\n",
    "  return ['ham' if ind == 0 else 'spam' for ind in vector_of_indices]\n",
    "\n",
    "# STWÓRZ KLASYFIKATOR\n",
    "nb = None\n",
    "\n",
    "# WYTRENUJ KLASYFIKATOR\n",
    "\n",
    "print(\"Ile elementów testowych udało się poprawnie zaklasyfikować?\")\n",
    "# OBLICZ TRAFNOŚĆ\n",
    "accuracy = 0\n",
    "print(accuracy)\n",
    "print(\"Szczegółowy raport (per klasa)\")\n",
    "# testowanie klasyfikatora - szerokie podsumowanie uwzględniające miary: precision, recall, f1\n",
    "print(classification_report(labels_as_strings(test['label_num']), labels_as_strings(nb.predict(X_test_counts))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 4b (0.5 punktu)\n",
    "Po analizie szczegółowego raportu z zadania 4a - odpowiedz na poniższe pytania i zapisz odpowiedzi w komentarzu:**\n",
    "<ol>\n",
    "<li>Która miara mówi nam o tym, jak wiele spośród elementów uznanych za spam rzeczywiście jest spamem?</li>\n",
    "<li>Która miara mówi nam o tym, jak wiele spośród wszystkich elementów rzeczywiście będących spamem zostało wykrytych jako spam?</li>\n",
    "<li>Która kategoria została w ogólnym rozrachunku lepiej rozpoznana przez klasyfikator, jeśli zależy nam bardziej na tym, żeby klasyfikator, jeśli mówi, że coś należy do danej klasy, raczej się w tym nie mylił, niż żeby wykrył wszystkie elementy klasy?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# odp zad 4.1: \n",
    "# odp zad 4.2:\n",
    "# odp zad 4.3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn jest bardzo wdzięcznym narzędziem, w którym proces klasyfikacji możemy wykonać w zaledwie kilku linijkach. Bardzo przydatną klasą jest klasa Pipeline, która definiuje sekwencję kroków, które wykonujemy wywołując metodę fit().\n",
    "W naszym przypadku mamy dwa kroki:\n",
    "<ol>\n",
    "    <li>Wektoryzacja - zamienia dane zapisane w postaci tekstowej na macierz z wektorami bag-of-words.</li>\n",
    "    <li>Klasyfikacja - wytrenowanie klasyfikatora.</li>\n",
    "</ol>\n",
    "W zdefiniowanym obiekcie typu pipeline, i+1 element pipeline'u na wejściu dostaje dane z wyjścia i-tego elementu (Zatem nasz klasyfikator otrzyma dane przetworzone przez TfidfVectorizer). <br/>\n",
    "Metoda fit na wejściu przyjmuje listę dokumentów w formie tekstowej, oraz oczekiwane etykiety w formie liczbowej.\n",
    "<br/>\n",
    "Analogicznie w procesie klasyfikowania nowych tekstów z użyciem istniejącego modelu - metoda predict() wykona sekwencję kroków: wektoryzacja + klasyfikacja dla zadanej listy surowych tekstów). <br/>\n",
    "Zapoznaj się z poniższym kodem i uruchom go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "# ------------------- WCZYTANIE DANYCH -----------\n",
    "\n",
    "# wczytaj dane z pliku CSV\n",
    "full_dataset = pandas.read_csv('resources/spam-emails.csv', encoding='utf-8')\n",
    "# ponieważ nazwy kategorii zapisane są z użyciem stringów: \"ham\"/\"spam\",\n",
    "# wykonujemy mapowanie tych wartości na liczby, aby móc wykonać klasyfikację.\n",
    "full_dataset['label_num'] = full_dataset.label.map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# ustaw seed na 0, aby zapewnić powtarzalność eksperymentu\n",
    "np.random.seed(0)\n",
    "# wylosuj 70% wierszy, które znajdą się w zbiorze treningowym\n",
    "train_indices = np.random.rand(len(full_dataset)) < 0.7\n",
    "\n",
    "# wybierz zbior treningowy (70%)\n",
    "train = full_dataset[train_indices]\n",
    "# wybierz zbiór testowy (dopełnienie treningowego - 30%)\n",
    "test = full_dataset[~train_indices]\n",
    "\n",
    "# ------------------- STWORZENIE PIPELINE'U -----------\n",
    "\n",
    "# stwórzmy pipeline surowy tekst -> TFIDF vectorizer -> klasyfikator\n",
    "pipeline = Pipeline([\n",
    "  ('tfidf', TfidfVectorizer(max_df=0.4)),\n",
    "  ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# ------------------- TRANSFORMACJA I UCZENIE -----------\n",
    "\n",
    "# zwektoryzujmy dane i wytrenujmy klasyfikator na zbiorze treningowym\n",
    "pipeline.fit(train['text'], train['label_num'])\n",
    "\n",
    "# ------------------- KLASYFIKACJA PRZYKŁADOWEGO TEKSTU -----------\n",
    "\n",
    "text_to_predict = \"NEED TO FIND SOMETHING? ::FREE MORTGAGE QUOTE:: To be removed from this list, click here. \"\n",
    "if pipeline.predict([text_to_predict]) == 1:\n",
    "  detected = 'SPAM'\n",
    "else:\n",
    "  detected = 'HAM'\n",
    "print(f\"Tekst {text_to_predict}, zaklasyfikowany został jako: {detected}\")\n",
    "\n",
    "# ------------------- OCENA KLASYFIKATORA -----------\n",
    "accuracy = 100 * pipeline.score(test['text'], test['label_num'])\n",
    "print(f\"W zbiorze testowym {accuracy:.2f}% przypadków zostało poprawnie zaklasyfikowanych!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Zadanie 5 (1 punkt): dobór parametrów klasyfikacji\n",
    "Poniżej znajduje się kod tworzący pipeline składający się z dwóch elementów: TfidfVectorizera oraz klasyfikatora naiwnego Bayesa - MultinomialNB. Wektoryzator tworzy model bag-of-words, który uwzględnia jedynie 1000 najważniejszych słów w słowniku. W celu zastosowania stemmingu oraz lematyzatora w treningu i predykcji stoworzona została klasa TheTokenizer, która poza podziałem tekstu na słowa wykonuje również zadania normalizacji wg. ustalonych flag: **use_stemming, use_lemmatization, use_stopword_removal**. <br/>\n",
    "<strong>Zadanie 5a (0.5 punktu)</strong>: <br/>\n",
    "Zweryfikuj jak zmiana wartości flag **use_stemming, use_lemmatization, use_stopword_removal**, a co za tym idzie wykorzystanie lamatyzacji, stemmingu i usuwania najczęstszych słow wpływa na miary precision, recall i f1 stworzonego klasyfikatora. Wyniki zapisz w komentarzu. <span style=\"color: #ff0000\">(modyfikuj linie 16, 17, 18, komentarz - w kolejnej komórce)</span><br/>\n",
    "<strong> Zadanie 5b (0.5 punktu)</strong>: <br/>\n",
    "Ustaw flagi **use_stemming, use_lemmatization, use_stopword_removal** z linii 16,17 i 18 na False, i porównaj wartości precision recall i f1 dla klasyfikatora, ktory wykorzystuje CountVectorizer i takiego, który wykorzystuje TfidfVectorizer. Pozostaw parametr max_features=1000 w obu przypadkach. Który wektoryzator jest lepszy? <span style=\"color: #ff0000\">(modyfikuj linię 84)</span>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# wczytywanie danych\n",
    "# zbiór danych zawarty w Sklearn, który zawiera dane z 20 grup newsowych\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Aby zastosować lematyzację/stemming z NLTK,\n",
    "# musimy napisać własny tokenizator,\n",
    "# który podzieli tekst na słowa\n",
    "# i przekształci je na stemy/lematy.\n",
    "class TheTokenizer(object):\n",
    "  def __init__(self):\n",
    "    #czy stemować?\n",
    "    self.use_stemming = False\n",
    "    #czy lematyzować?\n",
    "    self.use_lemmatization = False\n",
    "    #czy usunąć słowa częste jak the, and, of itp.\n",
    "    self.use_stopword_removal = False\n",
    "\n",
    "    # Utwórz lematyzator oparty na wordnet\n",
    "    self.wnl = WordNetLemmatizer()\n",
    "    # Utwórz stemmer Portera\n",
    "    self.stemmer = PorterStemmer()\n",
    "    # załaduj listę ~100 najczęstszych słów (the, and, of, ...)\n",
    "    self.stopwords = set(stopwords.words('english'))\n",
    "\n",
    "  def __call__(self, doc):\n",
    "    # tokenizuj i ew. lematyzuj/stemuj/usuń stopwords w zależności od ustawionych flag\n",
    "    if not self.use_stemming and not self.use_lemmatization:\n",
    "      return [t for t in word_tokenize(doc) if self.allow(t)]\n",
    "    elif self.use_stemming and not self.use_lemmatization:\n",
    "      return [self.stem_token(t) for t in word_tokenize(doc) if self.allow(t)]\n",
    "    elif self.use_lemmatization and not self.use_stemming:\n",
    "      return [self.lemmatize_token(t, pos) for t, pos in pos_tag(word_tokenize(doc)) if self.allow(t)]\n",
    "\n",
    "  def stem_token(self, token):\n",
    "    return self.stemmer.stem(token)\n",
    "\n",
    "  def lemmatize_token(self, token, postag):\n",
    "    return self.wnl.lemmatize(token, self.get_wordnet_pos(postag))\n",
    "\n",
    "  def allow(self, token):\n",
    "    return not self.use_stopword_removal or token not in self.stopwords\n",
    "\n",
    "  def get_wordnet_pos(self, treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "      return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "      return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "      return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "      return wordnet.ADV\n",
    "    else:\n",
    "      return wordnet.NOUN\n",
    "\n",
    "# funkcja pomocnicza zamieniająca identyfikatory numeryczne na tekstowe\n",
    "def labels_as_strings(vector_of_indices):\n",
    "  return [dataset_train.target_names[ind] for ind in vector_of_indices]\n",
    "\n",
    "\n",
    "print(\"Pobieranie danych\")\n",
    "# lista kategorii, które chcemy analizować\n",
    "categories = [\n",
    "  'misc.forsale',\n",
    "  'soc.religion.christian',\n",
    "  'sci.space',\n",
    "  'talk.politics.guns',\n",
    "  'comp.graphics',\n",
    "  'sci.med',\n",
    "  'rec.motorcycles',\n",
    "  'sci.med',\n",
    "  'sci.electronics',\n",
    "  'talk.politics.misc',\n",
    "  'comp.sys.mac.hardware'\n",
    "]\n",
    "\n",
    "# pobieramy zbiór uczący (na nim będziemy trenować) dla wybranych kategorii.\n",
    "dataset_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# pobieramy zbiór testowy (na nim będziemy testować) dla wybranych kategorii\n",
    "dataset_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Tworzenie pipeline'u\")\n",
    "# stwórzmy pipeline surowy tekst -> vectorizer -> klasyfikator\n",
    "pipeline = Pipeline([\n",
    "  ('vectorizer', TfidfVectorizer(tokenizer=TheTokenizer(), max_features=1000)),\n",
    "  ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "print(\"Uczenie pipeline'u\")\n",
    "# trenujemy klasyfikator!\n",
    "pipeline.fit(dataset_train.data, dataset_train.target)\n",
    "\n",
    "print(\"Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika?)?\")\n",
    "print(f\"W słowniku znajduje się {len(pipeline.named_steps['vectorizer'].vocabulary_.keys())} różnych słów\")\n",
    "\n",
    "print(\"Ocena klasyfikatora\")\n",
    "# testowanie klasyfikatora - szerokie podsumowanie uwzględniające miary: precision, recall, f1\n",
    "print(\n",
    "  classification_report(labels_as_strings(dataset_test.target), labels_as_strings(pipeline.predict(dataset_test.data))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}