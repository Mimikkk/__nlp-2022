{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "colab": {
   "name": "Zadania_lab3.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEdmAZsqklfo"
   },
   "source": [
    "# Modele językowe - n-gramy \n",
    "\n",
    "---\n",
    "\n",
    "## 1. N-gramy słów w klasyfikacji\n",
    "Poniżej stworzono kod, który przeprowadza klasyfikację dokumentów należących do 4 kategorii. W odróżnieniu do poprzednich zajęć - tu zaproponowano klasyfikator SVC (algorytm SVM, popularna alternatywa dla NaiveBayes), która również świetnie się spisuje w problemach klasyfikacji tekstu.\n",
    "\n",
    "**<span style=\"color: red\">Zadanie 1a (0.5 punktu)</span>** Uruchom kod, przyjrzyj się wygenerowanym wynikom (a najlepiej zachowaj je gdzieś, będą potrzebne). <br/>\n",
    "Zapoznaj się z dokumentacją TfIdfVectorizer, odnajdź opcję uwzględnienia nie tylko pojedynczych słów jako cechy, ale także ich par i **zmodyfikuj poniższy kod tak, aby klasyfikacja uwzględniała zarówno pojedyncze słowa jak i pary (pozostaw parametr max_df=0.1 nienaruszony).** <span style=\"color: red\"> Zmodyfikuj linię 30.</span><br/> <br/>\n",
    "**<span style=\"color: red\">Zadanie 1b (0.5 punktu)</span>** Jak zmieniła się liczba cech po uwzględnieniu tych par? Czy coś się zmieniło w raporcie z klasyfikacji? Uzupełnij odpowiedzi na pytania w komórce poniżej kodu."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SO0jQYNKklfw"
   },
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "# zbiór danych zawarty w Sklearn, który zawiera dane z 20 grup newsowych\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "# ------------------- WCZYTANIE DANYCH -----------\n",
    "# ustaw seed na 0, aby zapewnić powtarzalność eksperymentu\n",
    "np.random.seed(0)\n",
    "\n",
    "# kategorie do analizy\n",
    "categories = ['sci.space', 'comp.graphics', 'talk.politics.misc', 'comp.sys.mac.hardware']\n",
    "\n",
    "# pobieramy zbiór uczący/testowy (na nim będziemy trenować) dla wybranych kategorii.\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "# ------------------- STWORZENIE PIPELINE'U -----------\n",
    "# stwórzmy pipeline surowy tekst -> TFIDF vectorizer -> klasyfikator\n",
    "pipeline = Pipeline([\n",
    "  ('tfidf', TfidfVectorizer(max_df=0.1, ngram_range=(1, 2))),\n",
    "  ('clf', SVC(C=1.0, kernel='linear')),\n",
    "])\n",
    "\n",
    "# ------------------- TRANSFORMACJA I UCZENIE -----------\n",
    "# zwektoryzujmy dane i wytrenujmy klasyfikator na zbiorze treningowym\n",
    "pipeline.fit(train.data, train.target)\n",
    "\n",
    "print(f\"W słowniku znajduje się {len(pipeline.named_steps['tfidf'].vocabulary_.keys())} różnych cech\")\n",
    "\n",
    "# ------------------- OCENA KLASYFIKATORA -----------\n",
    "# testowanie klasyfikatora - szerokie podsumowanie uwzględniające miary:\n",
    "#  - precision,\n",
    "#  - recall,\n",
    "#  - f1\n",
    "print(classification_report(test.target, pipeline.predict(test.data)))"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W słowniku znajduje się 287718 różnych cech\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.91       389\n",
      "           1       0.92      0.94      0.93       385\n",
      "           2       0.95      0.90      0.92       394\n",
      "           3       0.97      0.90      0.93       310\n",
      "\n",
      "    accuracy                           0.92      1478\n",
      "   macro avg       0.93      0.92      0.92      1478\n",
      "weighted avg       0.92      0.92      0.92      1478\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<pre>\n",
    "W słowniku znajduje się 34774 różnych cech\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.88      0.93      0.90       389\n",
    "           1       0.91      0.93      0.92       385\n",
    "           2       0.94      0.91      0.92       394\n",
    "           3       0.96      0.90      0.93       310\n",
    "\n",
    "    accuracy                           0.92      1478\n",
    "   macro_avg       0.92      0.92      0.92      1478\n",
    "weighted_avg       0.92      0.92      0.92      1478\n",
    "</pre>\n",
    "<pre>\n",
    "W słowniku znajduje się 287718 różnych cech\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.94      0.91       389\n",
    "           1       0.92      0.94      0.93       385\n",
    "           2       0.95      0.90      0.92       394\n",
    "           3       0.97      0.90      0.93       310\n",
    "\n",
    "    accuracy                           0.92      1478\n",
    "   macro avg       0.93      0.92      0.92      1478\n",
    "weighted avg       0.92      0.92      0.92      1478\n",
    "</pre>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. O ile zwiększyła się liczba cech w klasyfikatorze?\n",
    "2. Czy precyzja w którejkolwiek klasie wzrosła? w której/których?\n",
    "3. Czy recall w którejkolwiek klasie wzrósł? w której/których?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Liczba cech w klasyfikatorze zwiększyła się o: 252944\n",
      "2. Precyzja w którejkolwiek klasie wzrosła w klasyfikatorze zawierającym unigramy i bigramy\n",
      "3. Recall lekko zwiększył się w klasyfikatorze zawierającym unigramy i bigramy\n"
     ]
    }
   ],
   "source": [
    "print(f\"1. Liczba cech w klasyfikatorze zwiększyła się o: {abs(287718 - 34774)}\")\n",
    "print(f\"2. Precyzja w którejkolwiek klasie wzrosła w klasyfikatorze zawierającym unigramy i bigramy\")\n",
    "print(f\"3. Recall lekko zwiększył się w klasyfikatorze zawierającym unigramy i bigramy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PX6uWjNklfy"
   },
   "source": [
    "## 2. N-gramy liter w klasyfikacji\n",
    "Poza n-gramami stworzonymi z następujących po sobie wyrazów - bardzo często używane są również n-gramy znakowe, stworzone z następujących po sobie liter. <br/><br/>\n",
    "Dla przykładu. wszystkie 3-gramy (trigramy) znakowe z napisu \"Hello world\" to: <br/>\n",
    "\"Hel\", \"ell\", \"llo\", \"lo \", \"o w\", \" wo\", \"wor\", \"orl\", \"rld\". <br/><br/>\n",
    "Do czego mogłaby być użyta taka reprezentacja tekstów? Okazuje się, że całkiem mocno pomaga to w rozwiązywaniiu problemu detekcji języka w którym został zapisany dokument, szczególnie w sytuacji, kiedy teksty są bardzo krótkie (np. tweety, smsy).\n",
    "<br/>\n",
    "Poniżej znajduje się szkielet klasyfikatora rozpoznającego język w którym zapisany jest dokument.\n",
    "Języków jest 6: polski, angielski, niemiecki, francuski, hiszpański i włoski.\n",
    "<br/>\n",
    "**<span style=\"color: red\">Zadanie 2 (1 punkt)</span>**: Przedstawiony klasyfikator jest znanym już z poprzednich przykładów kodem. Waszym zadaniem jest:\n",
    "<ol>\n",
    "    <li>Zapoznanie się dokumentacją Tf-Idf vectorizera, aby znaleźć funkcjonalność, która zamiast całych słów, stworzy cechy na podstawie liter i wykorzystanie tej funkcjonalności w kodzie</li>\n",
    "    <li>Ustawienie takiego zakresu n-gramów, aby zmaksymalizować uzyskany wynik (Oczekiwane 1.0 precyzji i recallu we wszystkich kategoriach przy pozostawieniu wartośi max_features = 300 elementów)</li>\n",
    "    <li>Poprawnie zaklasyfikuje krotki przykład zapisany w linii 43 (Bonjour przypisze do kategorii 'french').</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ULRDlJwzklfz"
   },
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "# -------------------- FUNKCJE POMOCNICZE --------\n",
    "\n",
    "# Funkcja mapująca identyfikator liczbowy kategorii na wartość tekstową, np: 0->\"polish\", 1->\"english\"\n",
    "def get_class_name_from_id(ids, mapping): return [mapping[id_] for id_ in ids]\n",
    "# ------------------- WCZYTANIE DANYCH -----------\n",
    "# wczytaj dane z pliku CSV\n",
    "full_dataset = pandas.read_csv('resources/language-detection-1000.csv', encoding='utf-8')\n",
    "lang_to_id = {'polish': 0, 'english': 1, 'french': 2, 'german': 3, 'italian': 4, 'spanish': 5}\n",
    "id_to_lang = {v: k for k, v in lang_to_id.items()}\n",
    "# Ponieważ nazwy kategorii zapisane są z użyciem stringów: \"ham\"/\"spam\",\n",
    "# wykonujemy mapowanie tych wartości na liczby, aby móc wykonać klasyfikację.\n",
    "full_dataset['label_num'] = full_dataset.lang.map(lang_to_id)\n",
    "\n",
    "# ustaw seed na 0, aby zapewnić powtarzalność eksperymentu\n",
    "np.random.seed(0)\n",
    "# wylosuj 70% wierszy, które znajdą się w zbiorze treningowym\n",
    "train_indices = np.random.rand(len(full_dataset)) < 0.7\n",
    "\n",
    "# Wybierz zbiór treningowy (70%)\n",
    "train = full_dataset[train_indices]\n",
    "# Wybierz zbiór testowy (dopełnienie treningowego - 30%)\n",
    "test = full_dataset[~train_indices]\n",
    "\n",
    "# ------------------- STWORZENIE PIPELINE'U -----------\n",
    "# stwórzmy pipeline surowy tekst -> TFIDF vectorizer -> klasyfikator\n",
    "pipeline = Pipeline([\n",
    "  ('tfidf', TfidfVectorizer(max_features=300, analyzer='char_wb', ngram_range=(1, 5))),\n",
    "  ('scaler', StandardScaler(with_mean=False)),\n",
    "  ('clf', LogisticRegression()),\n",
    "])\n",
    "# ------------------- TRANSFORMACJA I UCZENIE -----------\n",
    "\n",
    "# zwektoryzujmy dane i wytrenujmy klasyfikator na zbiorze treningowym\n",
    "pipeline.fit(train['text'], train['label_num'])\n",
    "\n",
    "print(\"Oto kilka przykładowych cech stworzonych przez TfidfVectorizer:\"\n",
    "      f\" {list(pipeline.named_steps['tfidf'].vocabulary_.keys())[:5]}\")\n",
    "\n",
    "# ------------------- WERYFIKACJA NA KRÓTKIM TEKŚCIE ----\n",
    "text_to_predict = \"Bonjour!\"\n",
    "predicted = pipeline.predict([text_to_predict])\n",
    "print(f\"\\n\\nTekst: {text_to_predict} został zaklasyfikowany jako: {id_to_lang[predicted[0]]}\\n\\n\")\n",
    "\n",
    "# ------------------- OCENA KLASYFIKATORA -----------\n",
    "print(classification_report(\n",
    "  get_class_name_from_id(test['label_num'], id_to_lang),\n",
    "  get_class_name_from_id(pipeline.predict(test['text']), id_to_lang)\n",
    "))"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hououin kyouma\\pycharmprojects\\nlp-laboratories\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oto kilka przykładowych cech stworzonych przez TfidfVectorizer: [' ', 'a', 'p', 'e', 'l']\n",
      "\n",
      "\n",
      "Tekst: Bonjour! został zaklasyfikowany jako: french\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     english       1.00      0.99      1.00       303\n",
      "      french       1.00      1.00      1.00       280\n",
      "      german       0.99      1.00      1.00       337\n",
      "     italian       1.00      1.00      1.00       273\n",
      "      polish       1.00      1.00      1.00       291\n",
      "     spanish       1.00      1.00      1.00       299\n",
      "\n",
      "    accuracy                           1.00      1783\n",
      "   macro avg       1.00      1.00      1.00      1783\n",
      "weighted avg       1.00      1.00      1.00      1783\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2o4W7uRsklf0"
   },
   "source": [
    "Widzimy, że problem jest stosunkowo prosty. Po co zatem używać n-gramów znakowych? Aby zaoszczędzić pamięć i podołać sytuacjom, w których zbiór testowy składa się ze słów, które nie występują w korpusie uczącym. <br/>\n",
    "\n",
    "O ile wszystkich słów w tych 6 językach jest \"30078\", to trigramów znakowych jest już tylko \"15274\", a bigramów - \"2059\". W związku z tym: <ol>\n",
    "<li>Używając n-gramów znakowych często możemy ograniczyć liczbę cech</li>\n",
    "<li>N-gramy znakowe pomogą nam w sytuacjach, kiedy dane słowo nie wystąpiło w tekście uczącym. Jeśli opieramy uczenie na pełnych słowach i cały nasz tekst testowy to niewystępujące w korpusie uczącym - \"bonjour\", wtedy wektor BOW będzie zawierał same zera, przez co będzie miał problem z przydziałem do odpowiedniej klasy. <br/> N-gramy znakowe nawet jeśli nie napotkały danego słowa podczas analizy korpusu, to na podstawie budowy samego słowa są w stanie przewidywać do jakiego języka słowo należy. Np. cokolwiek zawierającego trigram \"żeb\" należeć będzie raczej do języka polskiego.</li>\n",
    "</ol>\n",
    "\n",
    "---\n",
    "\n",
    "### 2a - istotność cech\n",
    "\n",
    "Ponieważ w zadaniu 2 użyliśmy znanego z zajęć z przedmiotu \"Sztuczna Inteligencja\" klasyfikatora liniowego - regresji logistycznej, podejrzeć możemy jakie cechy najsilniej sugerują nam przynależność do danej klasy. Uruchom poniższy kod, aby zobaczyć jakie cechy są najważniejsze dla danych kategorii. Modyfikując parametry TfidfVectorizer możesz zobaczyć jakie słowa/ciągi znaków są najistotniejsze do detekcji danego języka."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "1F01zfofklf1"
   },
   "source": [
    "# Funkcja, z której użyciem możemy ocenić które cechy najsilniej skojarzone są z danymi klasami.\n",
    "# Wyświetli listę słów/n-gramów znakowych, których obecność najsilniej wpływa na przydział do danej klasy\n",
    "def language_indicators(feature_names, feature_importances, id_to_lang):\n",
    "  # iterujemy po macierzy feature_importances (wymiarów: język x cechy) wierszami (czyli język po języku)\n",
    "  for i, language in enumerate(feature_importances):\n",
    "    # Tworzymy skojarzenie nazw cech z wagami modelu\n",
    "    # (ponieważ używamy regresji logistycznej — każda cecha (słowo/n-gram)\n",
    "    # ma swoją wagę, która jest optymalizowana w procesie uczenia!\n",
    "    # Cechy z wysokimi wagami są ważne dla danej klasy.\n",
    "    # Każda klasa ma osobny model ze swoimi wagami!)\n",
    "    # posortujmy cechy skojarzone z wagami malejąco\n",
    "    scored_features = sorted(zip(feature_names, language), key=lambda x: x[1], reverse=True)\n",
    "    #zamieńmy identyfikator numeryczny kategorii na nazwę języka\n",
    "    print(f\"W rozpoznaniu języka {id_to_lang[i]} najważniejsze cechy to:\")\n",
    "    # wybierzmy po 5 najważniejszych cech (najwyższe wartości uczonych współczynników)\n",
    "    for feature, score in scored_features[:5]:\n",
    "      print(f\"\\t'{feature}': {score}\")\n",
    "\n",
    "# ------------------- WYŚWIETLENIE NAJWAŻNIEJSZYCH CECH DLA KAŻDEJ KATEGORII\n",
    "language_indicators(\n",
    "  # Pobierz nazwy cech\n",
    "  pipeline.named_steps['tfidf'].get_feature_names(),\n",
    "  # Pobierz wyuczone współczynniki\n",
    "  # (regresja logistyczna to stworzenie modelu opisanego wzorem y = e^(-wx - b),\n",
    "  # gdzie uczymy się współczynników w.\n",
    "  # Pole coef_ zawiera te współczynniki dla każdego języka z osobna)\n",
    "  pipeline.named_steps['clf'].coef_,\n",
    "  # mapowanie z identyfikatora numerycznego na pełną nazwę języka — zwiększa czytelność wygenerowanego raportu\n",
    "  id_to_lang\n",
    ")"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W rozpoznaniu języka polish najważniejsze cechy to:\n",
      "\t'j': 0.19901617632544774\n",
      "\t'ł': 0.18816129382371247\n",
      "\t'ę': 0.18320232502803763\n",
      "\t'k': 0.17359664559466248\n",
      "\t'y': 0.17051681188204693\n",
      "W rozpoznaniu języka english najważniejsze cechy to:\n",
      "\t'th': 0.23285452718562907\n",
      "\t' an': 0.22631352214163009\n",
      "\t' th': 0.22288205475537914\n",
      "\t'the': 0.2158537554985106\n",
      "\t'y ': 0.21250378502730857\n",
      "W rozpoznaniu języka french najważniejsze cechy to:\n",
      "\t'é': 0.3959984284874838\n",
      "\t''': 0.23530640345092413\n",
      "\t'ce': 0.23522657533847321\n",
      "\t't ': 0.22677827203788822\n",
      "\t'ou': 0.22428070168981273\n",
      "W rozpoznaniu języka german najważniejsze cechy to:\n",
      "\t'ei': 0.20423099191463112\n",
      "\t'der': 0.17450101370170895\n",
      "\t'en ': 0.17434925387262365\n",
      "\t'en': 0.1740649213430604\n",
      "\t' au': 0.16620582946503853\n",
      "W rozpoznaniu języka italian najważniejsze cechy to:\n",
      "\t'i ': 0.34884806893525017\n",
      "\t'o ': 0.2801741071699971\n",
      "\t'tt': 0.20436243361062018\n",
      "\t'zi': 0.20107410278801147\n",
      "\t'i': 0.2003319918014233\n",
      "W rozpoznaniu języka spanish najważniejsze cechy to:\n",
      "\t'os ': 0.292164898313285\n",
      "\t'ci': 0.2706839668976856\n",
      "\t'el ': 0.25471658667742014\n",
      "\t' de ': 0.25308619683960704\n",
      "\t'de ': 0.22191067772066936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hououin kyouma\\pycharmprojects\\nlp-laboratories\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT4vAt5Kklf2"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. N-gramy słów w generowaniu tekstu\n",
    "\n",
    "Innym, bardzo ciekawym zastosowaniem n-gramów jest możliwość generowania tekstu z użyciem tzw. łańcuchów Markova. Stwórzmy funkcję generującą n-gramy słów, aby później móc ją wykorzystać do tworzenia tekstów.\n",
    "\n",
    "**<span style=\"color: red\">Zadanie 3 (1 punkt)</span>** stwórz funkcję, która wygeneruje n-gramy słów zadanego stopnia n (n_gram_len). Aby podzielić zdanie na słowa nie musisz używać tokenizatora z biblioteki, na potrzeby zadania wystarczy uznać, że spacja oddziela poszczególne słowa.\n",
    "<br/>\n",
    "<br/>\n",
    "<div class=\"alert alert-success\">\n",
    "Oczekiwany rezultat dla zadanych danych: <br/><br/>[['The', 'big', 'brown'], ['big', 'brown', 'fox'], ['brown', 'fox', 'jumped'], ['fox', 'jumped', 'over'], ['jumped', 'over', 'the'], ['over', 'the', 'fence.']]\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ooeUA4Gbklf3"
   },
   "source": [
    "from typing import Iterable\n",
    "from typing import Literal\n",
    "\n",
    "def generate_ngrams(sentence: str, n: int, mode: Literal['word', 'char']) -> Iterable[list[str] | str]:\n",
    "  match mode:\n",
    "    case 'word':\n",
    "      items = sentence.split()\n",
    "      return (items[i:i + n] for i in range(len(items) - n + 1))\n",
    "    case 'char':\n",
    "      items = list(sentence)\n",
    "      return (''.join(items[i:i + n]) for i in range(len(items) - n + 1))\n",
    "list(generate_ngrams(\"The big brown fox jumped over the fence.\", 3, mode='word'))"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "[['The', 'big', 'brown'],\n ['big', 'brown', 'fox'],\n ['brown', 'fox', 'jumped'],\n ['fox', 'jumped', 'over'],\n ['jumped', 'over', 'the'],\n ['over', 'the', 'fence.']]"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p93tpUzklf4"
   },
   "source": [
    "Jeśli udało Ci się napisać funkcję get_word_ngrams — zapoznaj się z poniższym kodem i uruchom go, aby wytworzyć tekst!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2GrZxjouklf4"
   },
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def generate_ngram_markov(text: str, n: int, mode: Literal['word', 'char']) -> dict[str, Counter]:\n",
    "  def pair_context_last_word(items: list[str]) -> tuple[str, str]:\n",
    "    (*items, last) = items\n",
    "    match mode:\n",
    "      case 'word':\n",
    "        return (' '.join(items), last)\n",
    "      case 'char':\n",
    "        return (''.join(items), last)\n",
    "  markov_dict = defaultdict(list[str])\n",
    "\n",
    "  for (context, word) in map(pair_context_last_word, generate_ngrams(text, n, mode)):\n",
    "    markov_dict[context].append(word)\n",
    "\n",
    "  for context in markov_dict:\n",
    "    markov_dict[context] = Counter(markov_dict[context])\n",
    "  return markov_dict\n",
    "\n",
    "n_gram_len = 3\n",
    "with open(\"resources/polish-europarl.txt\", 'r', encoding='utf-8') as file:\n",
    "  text = file.read().lower()\n",
    "markov_dict = generate_ngram_markov(text, n_gram_len, mode='word')\n",
    "print(len(markov_dict.keys()))"
   ],
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274320\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnio co dwa miesiące od początku roku wyraźnie wypowiadaliśmy nasze obawy co do dalszych cięć bez jakichkolwiek planów inwestycyjnych z krajami trzecimi, a to polega na zapewnieniu, by rynek pracy jest bardzo niski, a stopień wykorzystania środków ue. ponieważ jest on uprawniony do decydowania o statusie, prawach i strukturach. centralizacja polegająca na połączeniu badań i innowacji w o wiele za mało. nie możemy realizować tego rodzaju usług. czy europa coś z systemem handlu uprawnieniami do emisji, by zagwarantować zgodność pierwszego europejskiego programu doprowadzi do pogorszenia warunków życia i zwiększający konsumpcję. unia innowacji może pomóc zwiększyć zdolność ue do przeglądu wydatków we wszystkich państwach członkowskich. jestem przekonany, że przystąpienie rumunii i bułgarii do obszaru swobodnego przemieszczania się w porównaniu do wszystkich jego aspektów. oto dlaczego naszym celem powinno być żadnych nieporozumień. poruszymy tę kwestię, będąc w strefie euro i pozostałych krajów ue. analizując progi wskaźników, trzeba wziąć pod uwagę cała debatę, chociaż komisja jasno przedstawiła już swoje stanowisko: jesteśmy przekonani, że dostawy zasobów naturalnych i zapewniać dostarczanie konsumentom informacji, tym samym mniej kosztowne procedury regulujące zatwierdzanie produktów, a przede wszystkim do nich mówi. inaczej mówiąc, stawiając takie żądanie jesteśmy wierni postanowieniom traktatu. a przy podejmowaniu decyzji w sprawie przyszłości wspólnej polityki bezpieczeństwa i współpracy w obszarze zdatności do lotu we wszystkich obszarach, jeśli piękne słowa na temat sytuacji w europie i poza nią, zmiany sytuacji w rosji zostanie uwolniony na znacznie szerszą skalę, z pewnością musi spełnić wymogi ue. podczas gdy rządy w holandii. nie głosowałem za przyjęciem przedmiotowego sprawozdania, w którym wyznaczamy sobie bardzo jasne sformułowania, lecz równocześnie należy wprowadzić rozróżnienie między mikro, małymi i średnimi przedsiębiorstwami (mśp). pozwoli to zmienić obecną absurdalną i niesprawiedliwą sytuację, w której proponuje się w niej cech szczególnych każdego z państw członkowskich, a którym jest przekonanie obywateli do pracy w mśp. zatem ósme ramy finansowe przewidywały zwiększenie środków na płatności. każdy, kto szuka konsensusu i podejmowania wspólnych i spójnych ram na następne pokolenia. po trzecie, nie powinny unikać dialogu ze społeczeństwem obywatelskim, co naraża na szwank życie i które w całości procedurą sądową i rządy tych krajów z funduszy strukturalnych. z tego powodu, a dokładnie obywatele bułgarscy, nie stanowią dla pani bardziej znaczącą rolę. to pani jest prezydencją rotacyjną. to pani jest jedna i nie możemy bowiem pozwolić, by wzrost w państwach członkowskich, powinna przejąć kontrolę nad zyskami z transportu gazu. w rzeczywistości komisja musi skoordynować realizację planu działania, mającego nam pomóc w walce z tym chciałem wyrazić swoje poparcie dla autorki rezolucji, pani poseł sanchez-schmidt odnosi się to konieczne, zamykać granice, jest obłędna. jej efektem będzie też rozróżniał przedstawicieli grup interesu, takich jak mobilność artystów, pracowników sektora kultury oraz dzieł sztuki, a po co to oznacza. mówiąc wprost, albo pakt stabilności i wzrostu, nie ma wielkich konsekwencji finansowych. wiele z nich w zasadzie pogwałcił prawa mniejszości i przedstawicieli opozycji przed przemocą i zastraszaniem. na piśmie - niniejszym udziela się dyrektorowi europejskiego centrum monitorowania narkotyków i narkomanii. ponadto w sytuacji, gdy bezpieczeństwo żywnościowe jest obecnie w ue wymaga odpowiedzialnego zachowania w mocy tylko gdyby był wzajemnie korzystny,\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "from itertools import islice\n",
    "\n",
    "def generate_sentence(start: str, n: int, appendices: int) -> str:\n",
    "  for _ in range(appendices):\n",
    "    context = \" \".join(start.split(\" \")[-n + 1:])\n",
    "    # sprawdźmy słowa, które są dozwolone jako następniki naszego kontekstu (context)\n",
    "    # i wybierzmy taki następnik,\n",
    "    # który zostanie wylosowany zgodnie z rozkładem stworzonym przez histogram.\n",
    "    index = randrange(sum(markov_dict[context].values()))\n",
    "    new_word = next(islice(markov_dict[context].elements(), index, None))\n",
    "    if not new_word: return start\n",
    "    start += f\" {new_word}\"\n",
    "  return start\n",
    "print(generate_sentence('Średnio co dwa', n_gram_len, 500))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wd4Jmapgklf4"
   },
   "source": [
    "---\n",
    "\n",
    "## 4. N-gramy znakowe w generowaniu tekstu\n",
    "\n",
    "W bardzo podobny sposób do zadania 3, możemy stworzyć model, który generować będzie tekst literka po literce. <br/>\n",
    "**<span style=\"color: red\">Zadanie 4 (1 punkt)</span>** stwórz funkcję, która wygeneruje n-gramy znakowe zadanego stopnia n (n_gram_len).\n",
    "<br/>\n",
    "<br/>\n",
    "<div class=\"alert alert-success\">\n",
    "Oczekiwany rezultat dla zadanych danych: ['The', 'he ', 'e b', ' bi', 'big', 'ig ', 'g b', ' br', 'bro', 'row', 'own', 'wn ', 'n f', ' fo', 'fox', 'ox ', 'x j', ' ju', 'jum', 'ump', 'mpe', 'ped', 'ed ', 'd o', ' ov', 'ove', 'ver', 'er ', 'r t', ' th', 'the', 'he ', 'e f', ' fe', 'fen', 'enc', 'nce', 'ce.']\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Egh-HRhaklf5"
   },
   "source": [
    "# Funkcja zdefiniowana wcześniej\n",
    "print(list(generate_ngrams(\"The big brown fox jumped over the fence.\", 3, mode='char')))"
   ],
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'he ', 'e b', ' bi', 'big', 'ig ', 'g b', ' br', 'bro', 'row', 'own', 'wn ', 'n f', ' fo', 'fox', 'ox ', 'x j', ' ju', 'jum', 'ump', 'mpe', 'ped', 'ed ', 'd o', ' ov', 'ove', 'ver', 'er ', 'r t', ' th', 'the', 'he ', 'e f', ' fe', 'fen', 'enc', 'nce', 'ce.']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYGnivfiklf5"
   },
   "source": [
    "Po stworzeniu funkcji **get_character_ngrams()** możemy uruchomić generator znakowy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qMzT3HLaklf5"
   },
   "source": [
    "n_gram_len = len(text)\n",
    "with open('resources/mister-theodore.txt', 'r', encoding='utf-8') as file:\n",
    "  text = file.read().lower()\n",
    "markov_dict = generate_ngram_markov(text, n_gram_len, mode='char')"
   ],
   "execution_count": 143,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U szlachty «kropić, kropić!»\n",
      "«ależ, najsłodszy jezu! trzeba raz rzecz skończy się do niej przez okiennic szpary,\n",
      "i zgasło. i wnet sierpy gromadnie dzwoniąc w tabakierę palcami chrząsnął,\n",
      "jak węgorz do odarcia: lecz nam *urodzonym*,\n",
      "nam wielmożnym, do złotych swobód wzwyczajonym!\n",
      "ach, bracie protazeńku» rzekł klucznik był podobny rysiowi rannemu,\n",
      "który dziś żółty, dawniej na dwory pańskie, lub na łowy.\n",
      "podczas uczty na chorze tym kapela stała,\n",
      "i w organ, i w rozlicznej barwy *surojadki* srebrzystych na pró\n",
      "U szlachty sąsiedzkiej gromada\n",
      "za gościnnymi stoły sędziego zakazy,\n",
      "w niebytność maćka zwykle usta do milczenie miał słuch bardzo czuły.\n",
      "sam gawęda, i lubił niezmiernego słońca\n",
      "i odbita o ciemne murawy wezgłowiu cieni\n",
      "jeszcze zbyt wcześniej wstawszy, piły kawę;\n",
      "teraz drugą dla siebie.\n",
      "\n",
      "    i to wiadomo czemu.\n",
      "zaczęli proces w ziemstwie i guberskim rządzie żyjesz, jesteś zuch na szpady: wyjdź ty bracie ryków.\n",
      "lub wiesz co, wyszlem kogo z nich opis zwycięstwie,\n",
      "prawie wszyscy — mieć na pogotowiu!»\n",
      "asesor wo\n"
     ]
    }
   ],
   "source": [
    "text = 'U szlachty'\n",
    "for _ in range(500):\n",
    "  context = text[-n_gram_len + 1:]\n",
    "  index = randrange(sum(markov_dict[context].values()))\n",
    "  text += next(islice(markov_dict[context].elements(), index, None))\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZo5XHlmklf5"
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Ngramy do generowania tekstu - długość ngramu a jakość tekstu\n",
    "<span style=\"color: red\">**Zadanie 5 (1 punkt)**</span>\n",
    "Obswerując wyniki z zadań 3 i 4 i sprawdzając różne długości n-gramów (znakowych i słów) zastanów się:\n",
    "<ol>\n",
    "<li>Jakie ryzyko w kontekście jakości tekstu niesie ze sobą tworzenie tekstu z bardzo **krótkich** n-gramów?</li>\n",
    "<li>Jakie ryzyko w kontekście jakości tekstu niesie ze sobą tworzenie tekstu z bardzo **długich** n-gramów?</li>\n",
    "</ol>\n",
    "Odpowiedzi zawrzyj w komentarzu poniżej"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "4zrSMJRJklf6"
   },
   "source": [
    "# Zad 5:\n",
    "#  Pytanie 1: Wygenerowane teksty z krótkich ngramów bardzo łatwo zapominają o kontekście, przez co tracą spójność\n",
    "#  Pytanie 2: Wygenerowane teksty z długich ngramów mogą nie znaleźć godnego następnika, przez co mają prawdopodobieństwo równe 0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB6xEAwIklf6"
   },
   "source": [
    "---\n",
    "\n",
    "## 6. Bonus: Prawdopodobieństwo wystąpienia zdania - bez punktów\n",
    "Dodatkowo ciekawym zastosowaniem n-gramów jest również ocena - jak bardzo prawdopodobnym jest wystąpienie danego zdania w rzeczywistości. Kiedy rozwiązujemy zadanie translacji mowy na tekst, spotykamy się z sytuacjami, w których nie do końca wiemy, czy słowo, które zostało wypowiedziane to np. \"morze\" czy \"może\". Model językowy oparty o n-gramy może ocenić szansę wystąpienia danego ciągu wyrazów, a więc również wybrać bardziej prawdopodobny ciąg wyrazów w danym języku. <br/>\n",
    "\n",
    "Biorąc pod uwagę, że zdanie to ciąg wyrazów    $w_1, w_2, w_3, ..., w_n$\n",
    "Możemy poczynić upraszczające założenie, że aktualne słowo zależne jest jedynie od słowa poprzedniego, zatem prawdopodobieństwo wystąpienia zdania $P(sentence) = p(w_1|beginOfSentence)*p(w_2|w_1)*p(w_3|w_2)*...*p(w_n|w_(n-1))$\n",
    "\n",
    "Obliczając prawdopodobieństwa warunkowe, może się okazać, że w testowanym przez nas zdaniu mogą wystąpić dwie problematyczne sytuacje:\n",
    "<ol>\n",
    "    <li>słowo konteksowe $w_c$ ze wzoru $p(w_n|w_c)$ nie występuje w korpusie - bardzo mała szansa jeśli korpus jest wystarczająco duży</li>\n",
    "    <li>słowo następujące po kontekstowym ($w_n$) nie współwystępuje w korpusie ze słowem kontekstowym (więc $p(w_n|w_c) = 0$ - całkiem możliwy stan, dość łatwo można sobie wyobrazić sytuację braku współwystępowania pewnych słów nawet przetwarzając duży korpus</li>\n",
    "</ol>\n",
    "\n",
    "Aby poradzić sobie z sytuacją, w której chcemy aby pewne słowo rozpoczynało/kończyło tekst, możemy dodać sztuczne znaczniki początku (BOS - Begin of Sentence) i końca (EOS - End of Sentence) zdania. Wprowadzając te znaczniki, będziemy mogli obliczyć prawdopodobieństwo wystąpienia słowa, pod warunkiem, że rozpoczyna ono zdanie $p(w_n|BOS)$\n",
    "\n",
    "Poniżej znajduje się kod oceniający prawdopodobieństwo wystąpienia zdań. Widzimy, że jedno z tych zdań ma sensowniejszy tekst i chcielibyśmy, aby komputer był w stanie wybrać sensowniejszą opcję.\n",
    "\n",
    "Problematyczne sytuacje rozwiązane zostały następująco:\n",
    "<ol>\n",
    "<li>Jeśli brak słowa kontekstowego w wygenerowanym modelu - uznaj, że prawdopodobieństwo całego zania wynosi 0</li>\n",
    "<li>Jeśli słowo następujące po kontekstowym nie współwystępuje z kontekstowym - użyj wygładzania aby ustawić prawdopodobieństwo na bardzo małą wartość (ale niezerową)</li>\n",
    "</ol>\n",
    "\n",
    "**Zapoznaj się z kodem i urochom go, tutaj nie trzeba nic zmieniać, to jedynie wizualizacja zastosowania. Uwaga - aby poprawnie oszacować prawdopodobieństwa potrzeba posiadać wykonane zadanie 3 (stworzona funkcja get_word_ngrams)**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5-mX7pfoklf6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# tekst do oceny\n",
    "text1 = \"i heard that the european union is a valuable concept.\"\n",
    "# tekst do oceny\n",
    "text2 = \"i had that the euro bean union is a variable concept.\"\n",
    "\n",
    "# będziemy dzielić na zdania\n",
    "from nltk import sent_tokenize\n",
    "# i czyścić tekst\n",
    "import re\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# słownik zawierający częstości występowania słów w zależności od poprzedzającego je słowa\n",
    "markov_dict = defaultdict(list)\n",
    "\n",
    "def clean_text(text):\n",
    "  # czyszczenie tekstu ze znaków nowej linii, tabulatorów, spacji (wielokrotnych)\n",
    "  return re.sub(\"[\\n\\t ]+\", \" \", text)\n",
    "\n",
    "def make_begin_end_of_sentences(text):\n",
    "  result = \"\"\n",
    "  sentences = sent_tokenize(text)\n",
    "  for sent in sentences:\n",
    "    # dla każdego zdania dodajemy specjalne tagi <BOS> = begin of sentence oraz <EOS> - end of sentence\n",
    "    result += \" <BOS> {s} <EOS> \".format(s=sent)\n",
    "  return clean_text(result)\n",
    "\n",
    "def get_sentence_probability(sentence, markov_dict):\n",
    "  sentence = \" <BOS> {s} <EOS> \".format(s=sentence)\n",
    "  sentence = clean_text(sentence)\n",
    "\n",
    "  sentence = sentence.split(' ')\n",
    "  prob = 1.0\n",
    "  for i in range(len(sentence)):\n",
    "    if i < 1:\n",
    "      continue\n",
    "\n",
    "    # słowo poprzedzające\n",
    "    context = sentence[i - 1]\n",
    "    # aktualne słowo\n",
    "    word = sentence[i]\n",
    "\n",
    "    # jeśli słowo kontekstowe występuje w modelu - OK\n",
    "    if context in markov_dict.keys():\n",
    "      # jeśli słowo 'word' współwystępowało z\n",
    "      # 'context' w korpusie - obliczmy prawdopodobieństwo tej sytuacji p(wn|wc)\n",
    "      if word in markov_dict[context].keys():\n",
    "        prob *= 1.0 * markov_dict[context][word] / sum(markov_dict[context].values())\n",
    "      else:\n",
    "        # smoothing, jeśli dane slowo 'word' nie występowało\n",
    "        # po słowie 'context' w korpusie,\n",
    "        # ustalmy wartość prawdopodobieństwa na bardzo neiwielką.\n",
    "        prob *= 1 / (sum(markov_dict[context].values()) + 1)\n",
    "  return prob\n",
    "\n",
    "\n",
    "with open('english_europarl.txt', 'r') as f:\n",
    "  text = clean_text(f.read().lower())\n",
    "  text = make_begin_end_of_sentences(text)\n",
    "\n",
    "  # wygeneruj wszystkie 2-gramy słów z korpusu\n",
    "  n_grams = generate_ngrams(text, 2)\n",
    "  # dla każdego n-gramu...\n",
    "  for n_gram in n_grams:\n",
    "    # weź przedostatnie słowo jako kontekst\n",
    "    context = n_gram[-2]\n",
    "    # weź ostatnie słowo jako kontekst\n",
    "    last_word = n_gram[-1]\n",
    "    # dopiszmy następniki, które występują w korpusie po kontekście\n",
    "    markov_dict[context].append(last_word)\n",
    "\n",
    "    # dla każdego kontekstu\n",
    "  for context in markov_dict.keys():\n",
    "    # stwórz histogram słów jakie występują w korpusie po tym kontekście\n",
    "    markov_dict[context] = Counter(markov_dict[context])\n",
    "\n",
    "  # wyznacz prawdopodobieństwo wystąpienia text1\n",
    "  probability_of_sent1 = get_sentence_probability(text1, markov_dict)\n",
    "  # wyznacz prawdopodobieństwo wystąpienia text2\n",
    "  probability_of_sent2 = get_sentence_probability(text2, markov_dict)\n",
    "\n",
    "  print(\"Prawdopodobieństwo wystąpienia zdania 1: {p}\".format(p=get_sentence_probability(text1, markov_dict)))\n",
    "  print(\"Prawdopodobieństwo wystąpienia zdania 2: {p}\".format(p=get_sentence_probability(text2, markov_dict)))"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}