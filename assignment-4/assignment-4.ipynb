{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7977198918178164\n",
      "0.23409628697705598\n",
      "-0.48434715333575534\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "from typing import Any, TypeVar\n",
    "from numpy import array, ndarray, sqrt, square, sum, divide, multiply\n",
    "\n",
    "Shape = TypeVar('Shape')\n",
    "def cosine(first: list[float] | ndarray[Shape, float], second: list[float] | ndarray[Shape, float]) -> float:\n",
    "  first: ndarray[Any, float] = array(first, dtype=float)\n",
    "  second: ndarray[Any, float] = array(second, dtype=float)\n",
    "\n",
    "  return divide(sum(multiply(first, second)), sqrt(multiply(sum(square(first)), sum(square(second)))))\n",
    "\n",
    "\n",
    "print(cosine([1.0, 2.0, 3.0], [1.5, -0.7, -20]))\n",
    "print(cosine([-10.0, 17.0, 2.0], [5.3, 12.0, -20]))\n",
    "print(cosine([1.0, 2.0, 3.0], [1, -3000, 184]))\n",
    "print(cosine([1.0, 2.0, 3.0], [1, 2, 3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "**Oczekiwany rezultat:** <br/>\n",
    "<ul>\n",
    "<li>-0.797719891818</li>\n",
    "<li>0.234096286977</li>\n",
    "<li>-0.484347153336</li>\n",
    "<li>1.0</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za pomocą CountVectorizera stwórzmy reprezentację BOW dwóch krótkich dokumentów i sprawdźmy z użyciem stworzonej funkcji jakie jest ich podobieństwo. Uruchom poniższy kod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumenty reprezentowane jako BOW. Dokumenty w wierszach, słowa w kolumnach\n",
      "[[1 1 1 0 0]\n",
      " [1 1 1 1 1]]\n",
      "\n",
      "Podobieńśtwo dokumentów to\n",
      "0.7745966692414834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "doc1 = \"Ala ma kota\"\n",
    "doc2 = \"Ala ma pięknego puszystego kota\"\n",
    "docs = [doc1, doc2]\n",
    "\n",
    "X_train_counts = vectorizer.fit_transform(docs).todense()\n",
    "\n",
    "print(\"Dokumenty reprezentowane jako BOW. Dokumenty w wierszach, słowa w kolumnach\")\n",
    "print(X_train_counts)\n",
    "print(\"\\nPodobieńśtwo dokumentów to\")\n",
    "# tolist()[0] zamienia macierz 1xn na listę elementów 1xn\n",
    "print(cosine(X_train_counts[0].tolist()[0], X_train_counts[1].tolist()[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kiedy dokumenty zawierają te same słowa podobieństwo zostaje całkiem nieźle obliczone, co jednak, jeśli zamiast takich samych słów znajdziemy wyrazy bliskoznaczne? Sprawdźmy - uruchom poniższy kod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumenty reprezentowane jako BOW. Dokumenty w wierszach, słowa w kolumnach\n",
      "[[1 0]\n",
      " [0 1]]\n",
      "\n",
      "\n",
      "Podobieńśtwo dokumentów to\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "doc1 = \"kot\"\n",
    "doc2 = \"kotek\"\n",
    "docs = [doc1, doc2]\n",
    "\n",
    "print(\"Dokumenty reprezentowane jako BOW. Dokumenty w wierszach, słowa w kolumnach\")\n",
    "X_train_counts = vectorizer.fit_transform(docs).todense()\n",
    "print(X_train_counts)\n",
    "\n",
    "print(\"\\n\\nPodobieńśtwo dokumentów to\")\n",
    "print(cosine(X_train_counts[0].tolist()[0], X_train_counts[1].tolist()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Jak widzimy BOW nie radzi sobie z oceną podobieństwa synonimów. Dajmy zatem szansę embeddingom!\n",
    "\n",
    "---\n",
    "Embeddingi to nic innego jak osadzenie znaczenia słowa w n-wymiarowej przestrzeni tak, aby słowa podobne do siebie występowały blisko w tej n-wymiarowej przestrzeni. Możemy stworzyć je sami na podstawie dużego korpusu tekstu (co może być czasochłonne), z użyciem paczek takich jak: gensim (https://radimrehurek.com/gensim/) ale możemy również użyć \"pretrenowanych\" wektorów utworzonych już na jakimś korpusie, dostępnych np. pod adresem: (https://nlp.stanford.edu/projects/glove/). My wybierzemy opcję drugą - wykorzystania istniejących wektorów. <br/>\n",
    "\n",
    "Embeddingi dostarczone przez zespół Stanforda to pliki tekstowe postaci:<br/>\n",
    "słowo[SPACJA]wektor_liczb_oddzielonych_spacją_reprezentujących_znaczenie_slowa<br/>\n",
    "\n",
    "Funkcja ładująca embeddingi do pamięci została już stworzona. <br/>**Uruchom poniższy kod, aby móc wykrzystać tę funkcję w kolejnych komórkach i ocenić podobieństwo wyrazów bliskoznacznych.** <span style=\"color:red\">Uwaga: zmienna mapping bedzie wykorzystywana w kolejnych komorkach, więc aby była dla nich widoczna - poniższa komórka musi być koniecznie uruchomiona.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podobieństwo między kotem a kotkiem: 0.6386305647068641\n",
      "Podobieństwo między kotem a krzesłem: 0.29425297716624554\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_embeddings(path):\n",
    "  mapping = dict()\n",
    "\n",
    "  with open(path, 'r', encoding='utf8') as file:\n",
    "    for line in file:\n",
    "      line = line.strip()\n",
    "      split = line.split(\" \")\n",
    "      # stwórz słownik słowo -> wektor\n",
    "      mapping[split[0]] = np.array(split[1:], dtype=float)\n",
    "  return mapping\n",
    "\n",
    "# ładujemy wektory o długości 50 liczb\n",
    "mapping = load_embeddings('resources/glove.6B.50d.txt')\n",
    "\n",
    "kot = mapping['cat']\n",
    "kotek = mapping['kitten']\n",
    "krzeslo = mapping['chair']\n",
    "\n",
    "print(f\"Podobieństwo między kotem a kotkiem: {cosine(kot, kotek)}\")\n",
    "print(f\"Podobieństwo między kotem a krzesłem: {cosine(kot, krzeslo)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że embeddingi, które tutaj są dość krótkie (50 liczb opisujących każde słowo) dobrze oddają podobieństwo. Kot jest dużo bardziej podobny do kotka niż do krzesła!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2: Embeddingi - wizualizacja i podobieństwo\n",
    "\n",
    "Embeddingi są reprezentacją znaczenia słów w n-wymiarowej przestrzeni. Strona: http://projector.tensorflow.org próbuje zwizualizować tę przestrzeń poprzez rzutowanie pretrenowanych wektorów na przestrzeń 3-wymiarową.\n",
    "\n",
    "W celu wykonania zadania, otwórz powyższą stronę i wykonaj podpunkty a) i b)  <br/>\n",
    "**Zadanie 2a (0.5 punktu)**: Podaj 5 najbliższych słów do słowa \"data\" w domyślnie załadowanych embeddingach (Słowo, którego sąsiadów chcemy zlokalizować możemy wpisać w pole \"Search\" w górnej części po prawej stronie ekranu. Następnie, po wskazaniu słowa \"data\" ujrzymy listę najbardziej podobnych słów wraz z miarą podobieńtstwa. Listę najbardziej podobnych słów wraz z miarą odległości (wybierz cosinusową), zawrzyj w poniższym komentarzu.\n",
    "<br/>\n",
    "Uwaga - na stronie użyto odległości cosinusowej zamiast podobieństwa. Relacja pomiędzy obiema miarami jest bardzo prosta: odległość = 1 - podobieństwo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Słowo: register     Odległość: 0.554\n",
    "# 2. Słowo: file         Odległość: 0.548\n",
    "# 4. Słowo: instructions Odległość: 0.512\n",
    "# 3. Słowo: files        Odległość: 0.528\n",
    "# 5. Słowo: information  Odległość: 0.443"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widzimy, słowa, które są najbliższe słowu \"data\" to w tym przypadku wyrazy bliskoznaczne. <br/>\n",
    "**Zadanie 2b (0.5 punktu)**: Podaj 5 najbliższych słów do słowa \"red\". Czy nadal mamy do czynienia z synonimami?\n",
    "Odpowiedz na pytanie: jak można interpretować najbardziej podobne słowa(w jakim aspekcie są one podobne), skoro jak widać nie są to synonimy (przypomnij sobie zasadę działania embeddingów)? Odpowiedzi zawrzyj w komentarzach poniżej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Słowo: black  Odległość: 0.494\n",
    "# 2. Słowo: green  Odległość: 0.398\n",
    "# 3. Słowo: white  Odległość: 0.395\n",
    "# 4. Słowo: yellow Odległość: 0.382\n",
    "# 5. Słowo: blue   Odległość: 0.335\n",
    "\n",
    "# Interpretacja najbliższych słów: Czarny często występuje w parze z kolorem czerwonym oraz jest często traktowany też jako przeciwieństwo niebieskiego, ale nadal są kolorami więc istnieje powiązanie między nimi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 3: Relacje między wektorami \n",
    "\n",
    "Embeddingi zawierają w sobie informacje o znaczeniu. Co więcej - są wektorami, przez co możemy wykonywać na nich operacje (dodawanie, odejmowanie,...). Sprawdźmy jakie efekty uzyskamy wykonując operacje na tych wektorach.\n",
    "<br/>\n",
    "Interesować nas będzie efekt operacji: $\\vec{italy} - \\vec{rome} + \\vec{warsaw}$. Na co wskazywać będzie tak zdefiniowany wektor?<br/>\n",
    "Ponieważ wynikiem tej operacji będzie nowy wektor, napiszmy funkcję, która sprawdzi jakie istniejące słowo leży najbliżej tego wektora.\n",
    "\n",
    "**Zadanie 3:** Wypełnij funkcję get_most_similar() tak, aby dla zadanego wektora vec1, zwróciła słowo, którego wektor jest najbardziej podobny do wektora vec1 (do oceny podobieńśtwa wykorzystaj stworzoną w zad. 1 funkcję cosine). Parametrem embeddings jest słownik, który mapuje słowo na odpowiadający mu wektor.<br/>\n",
    "Jakie słowo zostało wyznaczone jako najbliższe do obliczonego punktu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poland\n"
     ]
    }
   ],
   "source": [
    "from typing import TypeVar\n",
    "\n",
    "Shape = TypeVar('Shape')\n",
    "def get_most_similar(vector: ndarray[Shape, float], embeddings: dict[str, ndarray[Shape, float]]) -> str:\n",
    "  return max(embeddings, key=lambda x: cosine(vector, embeddings[x]))\n",
    "\n",
    "new_point = mapping['italy'] - mapping['rome'] + mapping['warsaw']\n",
    "print(get_most_similar(new_point, mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13434   -0.55924   -0.061269   0.12031   -0.059634   1.119708\n",
      "  0.706171   0.47393   -0.74465   -1.045956   1.318756  -2.02057\n",
      " -0.42257    0.02422    0.35484    0.26507    0.615026   0.5488\n",
      " -0.46353    0.53853    0.871282  -0.857051  -0.85398    0.93783\n",
      " -0.346134  -1.4091     0.426467   0.11677   -0.25116    0.223392\n",
      "  1.75266    0.9151     0.236205   0.06239   -1.33426   -0.178076\n",
      " -0.54741    1.24018   -0.19442   -0.35939    0.42025    0.1212903\n",
      "  1.30291   -2.24267   -0.33217    0.9972    -0.14806    0.1206\n",
      " -0.4379    -0.53486  ]\n"
     ]
    }
   ],
   "source": [
    "print(new_point)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy zatem, że wykonywanie operacji na embeddingach pozwala na uzyskanie bardzo ciekawych rezulatów. Jeśli od obiektu \"Włochy\" odejmiemy jego stolicę, a dodamy stolicę polski, to otrzymamy obiekt \"Polska\". Innymi słowy - odpowiadamy na pytanie: jakie słowo jest w takiej relacji do Polski, w jakiej jest Rzym do Włoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 4: Klasyfikacja\n",
    "\n",
    "Okazuje się również, że embeddingi użyteczne są w klasyfikacji, skutecznie redukując nam liczbę cech a także rozwiązując problem rzadkiej reprezentacji. Wróćmy do zadania klasyfikacji spamu znanego z jednych z poprzenich laboratoriów. W celu decydowania czy mamy do czynienia ze spamem, czy może hamem, chcielibyśmy użyć klasyfikatora SVC, który jako cechy przyjmie embeddingi. <br/>\n",
    "\n",
    "Ponieważ jednak embeddingi opisują pojedyncze słowa jako n-wymiarowe wektory, a w problemie klasyfikacji maili musimy reprezentować całe dokumenty w postaci wektorów - musimy zagregować informacje o wszystkich słowach w jednym wektorze cech.\n",
    "<br/>\n",
    "\n",
    "Jedną z zadziwiająco dobrze działających metod okazuje się być reprezentowanie całego dokumentu jako wektora, będącego \"środkiem ciężkości\" słów z których jest zbudowany. Wektor wynikowy jest wektorem n-wymiarowym (tak jak wektor każdego ze słów \"składowych\"), gdzie $i$-ta pozycja wektora posiada wartość będącą średnią arytmetyczną $i$-tych pozycji wektorów słów z danego dokumentu. UWAGA: może okazać się, że w pretrenowanych embeddingach słowo z dokumentu, który chcemy reprezentować jako wektor nie występuje. W takiej syturacji zignorujmy kompletnie to słowo (uznajmy, że go nie ma).\n",
    "<br/>\n",
    "<br/>\n",
    "**Zadanie 4 (1 punkt)**: Zaimplementuj funkcję documents_to_ave_embeddings(docs, embeddings) [ linia 21 ] przyjmującej dwa parametry: \n",
    "<ol>\n",
    "    <li>docs - lista dokumentów w formie tekstowej (lista stringów) do przetransformowania na wektory</li>\n",
    "    <li>embeddings - mapowanie słowo -> wektor (embedding) z istniejącego modelu</li>\n",
    "</ol>\n",
    "Funkcja powinna zwrócić jedną zmienną, listę wektorów dokumentów, gdzie 1 dokument jest \"średnim wektorem\" wektorów słów z których jest zbudowany (jak w paragrafie powyżej treści zadania). Użyj tokenizatora z NLTK (word_tokenize) i przed stokenizowaniem - zamień teksty dokumentów na składające się z samych małych liter.\n",
    "\n",
    "Rozważ użycie numpy.mean z odpowiednią wartością parametru axis do policzenia średniej (będzie łatwiej użyć gotowej funkcji niż ręcznie implementować)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W zbiorze testowym 72.57844474761255% przypadków zostało poprawnie zaklasyfikowanych!\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.97      0.83       517\n",
      "           1       0.65      0.15      0.25       216\n",
      "\n",
      "    accuracy                           0.73       733\n",
      "   macro avg       0.69      0.56      0.54       733\n",
      "weighted avg       0.71      0.73      0.66       733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable\n",
    "from numpy import vectorize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ------------------- WCZYTANIE DANYCH -----------\n",
    "\n",
    "# wczytaj dane z pliku CSV\n",
    "full_dataset = pandas.read_csv('resources/spam-emails.csv', encoding='utf-8')\n",
    "# ponieważ nazwy kategorii zapisane są z użyciem stringów: \"ham\"/\"spam\", wykonujemy mapowanie tych wartości na liczby, aby móc wykonać klasyfikację.\n",
    "full_dataset['label_num'] = full_dataset.label.map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# ustaw seed na 0, aby zapewnić powtarzalność eksperymentu\n",
    "np.random.seed(0)\n",
    "# wylosuj 70% wierszy, które znajdą się w zbiorze treningowym\n",
    "train_indices = np.random.rand(len(full_dataset)) < 0.7\n",
    "\n",
    "# wybierz zbiór treningowy (70%)\n",
    "train = full_dataset[train_indices]\n",
    "# wybierz zbiór testowy (dopełnienie treningowego - 30%)\n",
    "test = full_dataset[~train_indices]\n",
    "\n",
    "def documents_to_ave_embeddings(docs: ndarray, embeddings: dict[str, ndarray]):\n",
    "  return array([[array([*map(embeddings.get, filter(embeddings.__contains__, words))]).mean()]\n",
    "                for words in map(word_tokenize, map(str.lower, docs))])\n",
    "\n",
    "# ------------------- STWORZENIE PIPELINE'U -----------\n",
    "\n",
    "train_transformed = documents_to_ave_embeddings(train['text'], mapping)\n",
    "test_transformed = documents_to_ave_embeddings(test['text'], mapping)\n",
    "\n",
    "# ------------------- TRANSFORMACJA I UCZENIE -----------\n",
    "\n",
    "# zwektoryzujmy dane i wytrenujmy klasyfikator na zbiorze treningowym\n",
    "classifier = SVC(C=1.0)\n",
    "classifier.fit(train_transformed, train['label_num'])\n",
    "\n",
    "# ------------------- OCENA KLASYFIKATORA -----------\n",
    "accuracy = classifier.score(test_transformed, test['label_num'])\n",
    "print(f\"W zbiorze testowym {100. * accuracy}% przypadków zostało poprawnie zaklasyfikowanych!\")\n",
    "# testowanie klasyfikatora - szerokie podsumowanie uwzględniające miary: precision, recall, f1\n",
    "print(classification_report(test['label_num'], classifier.predict(test_transformed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 5:  Odległość Levensteina (poprawianie literówek)\n",
    "Co prawda odległość Levensteina nie jest związana z koncepcją embeddingów, jednak do tej pory nie próbowaliśmy zastosować tej metody w praktyce. Ostatnie zadanie tych laboratoriów będzie dotyczyło właśnie odległości edycyjnej. \n",
    "\n",
    "**Zadanie 5 (1 punkt)**: Zaimplementuj funkcję get_closest_word która poprawi literówki z użyciem odległości Levensteina. Jeśli aktualny token występuje w słowniku (words_set) - nie powinien zostać korygowany, jeśli zaś nie występuje - powinien być zamieniony na słowo, którego odległość Levensteina jest najmniejsza. Czy udało się poprawić ten zaszumiony tekst?  Wykorzystaj funkcję edit_distance z nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tekst oryginalny: dzięń dobry pańestwu, labolatolia pżeprowadzone bydą w sali omputerowej 1.6.10. pozdrawiem\n",
      "Tekst poprawiony: dzień dobry państwu , laboratoria przeprowadzone będą w sali komputerowej 1.6.10. pozdrawiam\n",
      "Czas poprawiania: 1.5073075000000244\n"
     ]
    }
   ],
   "source": [
    "# Dystans levensteina\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk import word_tokenize\n",
    "from timeit import default_timer as timer  # funkcja potrzebna do mierzenia czasu\n",
    "\n",
    "input_text = \"dzięń dobry pańestwu, labolatolia pżeprowadzone bydą w sali omputerowej 1.6.10. pozdrawiem\"\n",
    "\n",
    "# zbiór słów, które są poprawnymi\n",
    "with open('resources/slowa-min.txt', 'r', encoding='utf8') as f:\n",
    "  valid_words = set([w.strip() for w in f.read().split(\"\\n\") if w.strip() != ''])\n",
    "\n",
    "def get_closest_word(token, valid_words):\n",
    "  return min(valid_words, key=lambda x: edit_distance(token, x)) if token not in valid_words else token\n",
    "\n",
    "# tokenizuj\n",
    "tokenized_input = word_tokenize(input_text)\n",
    "# pobierz czas\n",
    "start = timer()\n",
    "# lista poprawionych tokenów\n",
    "corrected = []\n",
    "for token in tokenized_input:\n",
    "  # jeśli token nie jest słowem lub jest już w słowniku — nie poprawiaj\n",
    "  if not token.isalpha() or token in valid_words:\n",
    "    corrected.append(token)\n",
    "  else:\n",
    "    # popraw\n",
    "    corrected.append(get_closest_word(token, valid_words))\n",
    "end = timer()\n",
    "\n",
    "print(f\"Tekst oryginalny: {input_text}\")\n",
    "print(f\"Tekst poprawiony: {' '.join(corrected)}\")\n",
    "print(f\"Czas poprawiania: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Trenowanie własnych wektorów\n",
    "Ponieważ trenowanie wektorów na dużym korpusie może być czasochłonne i wymaga dużego korpusu, aby wyłapać odpowiednie konteksty słów, nie wykonywaliśmy go na laboratoriach. Zainteresowanym tworzeniem własnych wektorów polecam artykuł: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/. Który opisuje jak wytrenować embeddingi z użyciem pythona i paczki \"gensim\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}